{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from colorama import init\n",
    "from termcolor import colored\n",
    "\n",
    "import blink.ner as NER\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "from blink.biencoder.biencoder import BiEncoderRanker, load_biencoder\n",
    "from blink.crossencoder.crossencoder import CrossEncoderRanker, load_crossencoder\n",
    "from blink.biencoder.data_process import (\n",
    "    process_mention_data,\n",
    "    get_candidate_representation,\n",
    ")\n",
    "import blink.candidate_ranking.utils as utils\n",
    "from blink.crossencoder.train_cross import modify, evaluate\n",
    "from blink.crossencoder.data_process import prepare_crossencoder_data\n",
    "from blink.indexer.faiss_indexer import DenseFlatIndexer, DenseHNSWFlatIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a207a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd41095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        self.fc1 = nn.Linear(n, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = nn.Sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGHLIGHTS = [\n",
    "    \"on_red\",\n",
    "    \"on_green\",\n",
    "    \"on_yellow\",\n",
    "    \"on_blue\",\n",
    "    \"on_magenta\",\n",
    "    \"on_cyan\",\n",
    "]\n",
    "\n",
    "\n",
    "def _print_colorful_text(input_sentence, samples):\n",
    "    init()  # colorful output\n",
    "    msg = \"\"\n",
    "    if samples and (len(samples) > 0):\n",
    "        msg += input_sentence[0 : int(samples[0][\"start_pos\"])]\n",
    "        for idx, sample in enumerate(samples):\n",
    "            msg += colored(\n",
    "                input_sentence[int(sample[\"start_pos\"]) : int(sample[\"end_pos\"])],\n",
    "                \"grey\",\n",
    "                HIGHLIGHTS[idx % len(HIGHLIGHTS)],\n",
    "            )\n",
    "            if idx < len(samples) - 1:\n",
    "                msg += input_sentence[\n",
    "                    int(sample[\"end_pos\"]) : int(samples[idx + 1][\"start_pos\"])\n",
    "                ]\n",
    "            else:\n",
    "                msg += input_sentence[int(sample[\"end_pos\"]) :]\n",
    "    else:\n",
    "        msg = input_sentence\n",
    "        print(\"Failed to identify entity from text:\")\n",
    "    print(\"\\n\" + str(msg) + \"\\n\")\n",
    "\n",
    "\n",
    "def _print_colorful_prediction(\n",
    "    idx, sample, e_id, e_title, e_text, e_url, show_url=False\n",
    "):\n",
    "    print(colored(sample[\"mention\"], \"grey\", HIGHLIGHTS[idx % len(HIGHLIGHTS)]))\n",
    "    to_print = \"id:{}\\ntitle:{}\\ntext:{}\\n\".format(e_id, e_title, e_text[:256])\n",
    "    if show_url:\n",
    "        to_print += \"url:{}\\n\".format(e_url)\n",
    "    print(to_print)\n",
    "\n",
    "\n",
    "def _annotate(ner_model, input_sentences):\n",
    "    ner_output_data = ner_model.predict(input_sentences)\n",
    "    sentences = ner_output_data[\"sentences\"]\n",
    "    mentions = ner_output_data[\"mentions\"]\n",
    "    samples = []\n",
    "    for mention in mentions:\n",
    "        record = {}\n",
    "        record[\"label\"] = \"unknown\"\n",
    "        record[\"label_id\"] = -1\n",
    "        # LOWERCASE EVERYTHING !\n",
    "        record[\"context_left\"] = sentences[mention[\"sent_idx\"]][\n",
    "            : mention[\"start_pos\"]\n",
    "        ].lower()\n",
    "        record[\"context_right\"] = sentences[mention[\"sent_idx\"]][\n",
    "            mention[\"end_pos\"] :\n",
    "        ].lower()\n",
    "        record[\"mention\"] = mention[\"text\"].lower()\n",
    "        record[\"start_pos\"] = int(mention[\"start_pos\"])\n",
    "        record[\"end_pos\"] = int(mention[\"end_pos\"])\n",
    "        record[\"sent_idx\"] = mention[\"sent_idx\"]\n",
    "        samples.append(record)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def _load_candidates(\n",
    "    entity_catalogue, entity_encoding, faiss_index=None, index_path=None, logger=None\n",
    "):\n",
    "    # only load candidate encoding if not using faiss index\n",
    "    if faiss_index is None:\n",
    "        candidate_encoding = torch.load(entity_encoding)\n",
    "        indexer = None\n",
    "    else:\n",
    "        if logger:\n",
    "            logger.info(\"Using faiss index to retrieve entities.\")\n",
    "        candidate_encoding = None\n",
    "        assert index_path is not None, \"Error! Empty indexer path.\"\n",
    "        if faiss_index == \"flat\":\n",
    "            indexer = DenseFlatIndexer(1)\n",
    "        elif faiss_index == \"hnsw\":\n",
    "            indexer = DenseHNSWFlatIndexer(1)\n",
    "        else:\n",
    "            raise ValueError(\"Error! Unsupported indexer type! Choose from flat,hnsw.\")\n",
    "        indexer.deserialize_from(index_path)\n",
    "\n",
    "    # load all the 5903527 entities\n",
    "    title2id = {}\n",
    "    id2title = {}\n",
    "    id2text = {}\n",
    "    wikipedia_id2local_id = {}\n",
    "    local_idx = 0\n",
    "    with open(entity_catalogue, \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in lines:\n",
    "            entity = json.loads(line)\n",
    "\n",
    "            if \"idx\" in entity:\n",
    "                split = entity[\"idx\"].split(\"curid=\")\n",
    "                if len(split) > 1:\n",
    "                    wikipedia_id = int(split[-1].strip())\n",
    "                else:\n",
    "                    wikipedia_id = entity[\"idx\"].strip()\n",
    "\n",
    "                assert wikipedia_id not in wikipedia_id2local_id\n",
    "                wikipedia_id2local_id[wikipedia_id] = local_idx\n",
    "\n",
    "            title2id[entity[\"title\"]] = local_idx\n",
    "            id2title[local_idx] = entity[\"title\"]\n",
    "            id2text[local_idx] = entity[\"text\"]\n",
    "            local_idx += 1\n",
    "    return (\n",
    "        candidate_encoding,\n",
    "        title2id,\n",
    "        id2title,\n",
    "        id2text,\n",
    "        wikipedia_id2local_id,\n",
    "        indexer,\n",
    "    )\n",
    "\n",
    "\n",
    "def __map_test_entities(test_entities_path, title2id, logger):\n",
    "    # load the 732859 tac_kbp_ref_know_base entities\n",
    "    kb2id = {}\n",
    "    missing_pages = 0\n",
    "    n = 0\n",
    "    with open(test_entities_path, \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in lines:\n",
    "            entity = json.loads(line)\n",
    "            if entity[\"title\"] not in title2id:\n",
    "                missing_pages += 1\n",
    "            else:\n",
    "                kb2id[entity[\"entity_id\"]] = title2id[entity[\"title\"]]\n",
    "            n += 1\n",
    "    if logger:\n",
    "        logger.info(\"missing {}/{} pages\".format(missing_pages, n))\n",
    "    return kb2id\n",
    "\n",
    "\n",
    "def __load_test(test_filename, kb2id, wikipedia_id2local_id, logger, consider_all=False):\n",
    "    test_samples = []\n",
    "    with open(test_filename, \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in lines:\n",
    "            record = json.loads(line)\n",
    "            record[\"label\"] = str(record[\"label_id\"])\n",
    "\n",
    "            # for tac kbp we should use a separate knowledge source to get the entity id (label_id)\n",
    "            if kb2id and len(kb2id) > 0:\n",
    "                if record[\"label\"] in kb2id:\n",
    "                    record[\"label_id\"] = kb2id[record[\"label\"]]\n",
    "                else:\n",
    "                    if consider_all:\n",
    "                        # NIL\n",
    "                        record[\"label_id\"] = -1\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            # check that each entity id (label_id) is in the entity collection\n",
    "            elif wikipedia_id2local_id and len(wikipedia_id2local_id) > 0:\n",
    "                try:\n",
    "                    key = int(record[\"label\"].strip())\n",
    "                    if key in wikipedia_id2local_id:\n",
    "                        record[\"label_id\"] = wikipedia_id2local_id[key]\n",
    "                    else:\n",
    "                        if consider_all:\n",
    "                            # NIL\n",
    "                            record[\"label_id\"] = -1\n",
    "                        else:\n",
    "                            continue\n",
    "                except:\n",
    "                    if consider_all:\n",
    "                        # NIL\n",
    "                        record[\"label_id\"] = -1\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            # LOWERCASE EVERYTHING !\n",
    "            record[\"context_left\"] = record[\"context_left\"].lower()\n",
    "            record[\"context_right\"] = record[\"context_right\"].lower()\n",
    "            record[\"mention\"] = record[\"mention\"].lower()\n",
    "            test_samples.append(record)\n",
    "\n",
    "    if logger:\n",
    "        logger.info(\"{}/{} samples considered\".format(len(test_samples), len(lines)))\n",
    "    return test_samples\n",
    "\n",
    "\n",
    "def _get_test_samples(\n",
    "    test_filename, test_entities_path, title2id, wikipedia_id2local_id, logger, consider_all=False\n",
    "):\n",
    "    kb2id = None\n",
    "    if test_entities_path:\n",
    "        kb2id = __map_test_entities(test_entities_path, title2id, logger)\n",
    "    test_samples = __load_test(test_filename, kb2id, wikipedia_id2local_id, logger, consider_all=consider_all)\n",
    "    return test_samples\n",
    "\n",
    "\n",
    "def _process_biencoder_dataloader(samples, tokenizer, biencoder_params):\n",
    "    _, tensor_data = process_mention_data(\n",
    "        samples,\n",
    "        tokenizer,\n",
    "        biencoder_params[\"max_context_length\"],\n",
    "        biencoder_params[\"max_cand_length\"],\n",
    "        silent=True,\n",
    "        logger=None,\n",
    "        debug=biencoder_params[\"debug\"],\n",
    "    )\n",
    "    sampler = SequentialSampler(tensor_data)\n",
    "    dataloader = DataLoader(\n",
    "        tensor_data, sampler=sampler, batch_size=biencoder_params[\"eval_batch_size\"]\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def _run_biencoder(biencoder, dataloader, candidate_encoding, top_k=100, indexer=None, save_encodings=False):\n",
    "    biencoder.model.eval()\n",
    "    labels = []\n",
    "    nns = []\n",
    "    all_scores = []\n",
    "    encodings = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        context_input, _, label_ids = batch\n",
    "        with torch.no_grad():\n",
    "            if indexer is not None:\n",
    "                context_encoding = biencoder.encode_context(context_input).numpy()\n",
    "                context_encoding = np.ascontiguousarray(context_encoding)\n",
    "                if save_encodings:\n",
    "                    encodings.extend([e.tolist() for e in context_encoding])\n",
    "                print('encoding_shape', context_encoding.shape)\n",
    "                global my_enc\n",
    "                my_enc = context_encoding\n",
    "                scores, indicies = indexer.search_knn(context_encoding, top_k)\n",
    "            else:\n",
    "                scores = biencoder.score_candidate(\n",
    "                    context_input, None, cand_encs=candidate_encoding  # .to(device)\n",
    "                )\n",
    "                scores, indicies = scores.topk(top_k)\n",
    "                scores = scores.data.numpy()\n",
    "                indicies = indicies.data.numpy()\n",
    "\n",
    "        labels.extend(label_ids.data.numpy())\n",
    "        nns.extend(indicies)\n",
    "        all_scores.extend(scores)\n",
    "    return labels, nns, all_scores, encodings\n",
    "\n",
    "\n",
    "def _process_crossencoder_dataloader(context_input, label_input, crossencoder_params):\n",
    "    tensor_data = TensorDataset(context_input, label_input)\n",
    "    sampler = SequentialSampler(tensor_data)\n",
    "    dataloader = DataLoader(\n",
    "        tensor_data, sampler=sampler, batch_size=crossencoder_params[\"eval_batch_size\"]\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def _run_crossencoder(crossencoder, dataloader, logger, context_len, device=\"cuda\"):\n",
    "    crossencoder.model.eval()\n",
    "    accuracy = 0.0\n",
    "    crossencoder.to(device)\n",
    "\n",
    "    res = evaluate(crossencoder, dataloader, device, logger, context_len, zeshel=False, silent=False)\n",
    "    accuracy = res[\"normalized_accuracy\"]\n",
    "    logits = res[\"logits\"]\n",
    "\n",
    "    if accuracy > -1:\n",
    "        predictions = np.argsort(logits, axis=1)\n",
    "    else:\n",
    "        predictions = []\n",
    "\n",
    "    return accuracy, predictions, logits\n",
    "\n",
    "\n",
    "def load_models(args, logger=None):\n",
    "\n",
    "    # load biencoder model\n",
    "    if logger:\n",
    "        logger.info(\"loading biencoder model\")\n",
    "    with open(args.biencoder_config) as json_file:\n",
    "        biencoder_params = json.load(json_file)\n",
    "        biencoder_params[\"path_to_model\"] = args.biencoder_model\n",
    "    biencoder = load_biencoder(biencoder_params)\n",
    "\n",
    "    crossencoder = None\n",
    "    crossencoder_params = None\n",
    "    if not args.fast:\n",
    "        # load crossencoder model\n",
    "        if logger:\n",
    "            logger.info(\"loading crossencoder model\")\n",
    "        with open(args.crossencoder_config) as json_file:\n",
    "            crossencoder_params = json.load(json_file)\n",
    "            crossencoder_params[\"path_to_model\"] = args.crossencoder_model\n",
    "        crossencoder = load_crossencoder(crossencoder_params)\n",
    "\n",
    "    # load candidate entities\n",
    "    if logger:\n",
    "        logger.info(\"loading candidate entities\")\n",
    "    (\n",
    "        candidate_encoding,\n",
    "        title2id,\n",
    "        id2title,\n",
    "        id2text,\n",
    "        wikipedia_id2local_id,\n",
    "        faiss_indexer,\n",
    "    ) = _load_candidates(\n",
    "        args.entity_catalogue,\n",
    "        args.entity_encoding,\n",
    "        faiss_index=getattr(args, 'faiss_index', None),\n",
    "        index_path=getattr(args, 'index_path' , None),\n",
    "        logger=logger,\n",
    "    )\n",
    "    \n",
    "    nil_prediction_model_bi = None\n",
    "    nil_prediction_features_bi = []\n",
    "    nil_prediction_model = None\n",
    "    nil_prediction_features = []\n",
    "    \n",
    "    if (hasattr(args, 'nil_prediction')\n",
    "            and args.nil_prediction\n",
    "            #and hasattr(args, 'nil_prediction_scaler')\n",
    "            and hasattr(args, 'nil_prediction_model')\n",
    "            and hasattr(args, 'nil_prediction_features')\n",
    "        ):\n",
    "        #nil_prediction_scaler = _load_pickle_model(args.nil_prediction_scaler)\n",
    "        nil_prediction_model_bi = _load_pickle_model(args.nil_prediction_model_bi)\n",
    "        nil_prediction_features_bi = args.nil_prediction_features_bi\n",
    "        #nil_prediction_model = _load_torch_model(args.nil_prediction_model, len(args.nil_prediction_features))\n",
    "        nil_prediction_model = _load_pickle_model(args.nil_prediction_model)\n",
    "        nil_prediction_features = args.nil_prediction_features\n",
    "\n",
    "    return (\n",
    "        biencoder,\n",
    "        biencoder_params,\n",
    "        crossencoder,\n",
    "        crossencoder_params,\n",
    "        candidate_encoding,\n",
    "        title2id,\n",
    "        id2title,\n",
    "        id2text,\n",
    "        wikipedia_id2local_id,\n",
    "        faiss_indexer,\n",
    "        nil_prediction_model_bi,\n",
    "        nil_prediction_features_bi,\n",
    "        nil_prediction_model,\n",
    "        nil_prediction_features\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scores_get_stats(scores):\n",
    "    global bi_higher_is_better\n",
    "    scores = scores.tolist()\n",
    "    _stats = {\n",
    "        \"max\": max(scores),\n",
    "        \"second\": sorted(scores, reverse=bi_higher_is_better)[1],\n",
    "        \"min\": min(scores),\n",
    "        \"mean\": statistics.mean(scores),\n",
    "        \"median\": statistics.median(scores),\n",
    "        \"stdev\": statistics.stdev(scores),\n",
    "    }\n",
    "    return _stats\n",
    "\n",
    "def _load_pickle_model(path):\n",
    "    with open(path, 'rb') as fd:\n",
    "        mdl = pickle.load(fd)\n",
    "    return mdl\n",
    "\n",
    "def _load_torch_model(path, n):\n",
    "    model = binaryClassification(n)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    args,\n",
    "    logger,\n",
    "    biencoder,\n",
    "    biencoder_params,\n",
    "    crossencoder,\n",
    "    crossencoder_params,\n",
    "    candidate_encoding,\n",
    "    title2id,\n",
    "    id2title,\n",
    "    id2text,\n",
    "    wikipedia_id2local_id,\n",
    "    faiss_indexer=None,\n",
    "    nil_prediction_model_bi = None,\n",
    "    nil_prediction_features_bi = ['max_bi'],\n",
    "    nil_prediction_model = None,\n",
    "    nil_prediction_features = ['max_cross'],\n",
    "    test_data=None,\n",
    "    local_id2wikipedia_id=None\n",
    "):\n",
    "\n",
    "    if not test_data and not args.test_mentions and not args.interactive:\n",
    "        msg = (\n",
    "            \"ERROR: either you start BLINK with the \"\n",
    "            \"interactive option (-i) or you pass in input test mentions (--test_mentions)\"\n",
    "            \"and test entitied (--test_entities)\"\n",
    "        )\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    id2url = {\n",
    "        v: \"https://en.wikipedia.org/wiki?curid=%s\" % k\n",
    "        for k, v in wikipedia_id2local_id.items()\n",
    "    }\n",
    "\n",
    "    stopping_condition = False\n",
    "    while not stopping_condition:\n",
    "\n",
    "        samples = None\n",
    "\n",
    "        if args.interactive:\n",
    "            logger.info(\"interactive mode\")\n",
    "\n",
    "            # biencoder_params[\"eval_batch_size\"] = 1\n",
    "\n",
    "            # Load NER model\n",
    "            ner_model = NER.get_model()\n",
    "\n",
    "            # Interactive\n",
    "            text = input(\"insert text:\")\n",
    "\n",
    "            # Identify mentions\n",
    "            samples = _annotate(ner_model, [text])\n",
    "\n",
    "            _print_colorful_text(text, samples)\n",
    "\n",
    "        else:\n",
    "            if logger:\n",
    "                logger.info(\"test dataset mode\")\n",
    "\n",
    "            if test_data:\n",
    "                samples = test_data\n",
    "            else:\n",
    "                # Load test mentions\n",
    "                samples = _get_test_samples(\n",
    "                    args.test_mentions,\n",
    "                    args.test_entities,\n",
    "                    title2id,\n",
    "                    wikipedia_id2local_id,\n",
    "                    logger,\n",
    "                )\n",
    "\n",
    "            stopping_condition = True\n",
    "            \n",
    "        if len(samples) == 0:\n",
    "            return (\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                len(samples),\n",
    "                [],\n",
    "                [],\n",
    "            )\n",
    "\n",
    "        # don't look at labels\n",
    "        keep_all = (\n",
    "            args.interactive\n",
    "            or samples[0][\"label\"] == \"unknown\"\n",
    "            or samples[0][\"label_id\"] < 0\n",
    "            or (hasattr(args, 'keep_all') and args.keep_all)\n",
    "        )\n",
    "\n",
    "        # prepare the data for biencoder\n",
    "        if logger:\n",
    "            logger.info(\"preparing data for biencoder\")\n",
    "        dataloader = _process_biencoder_dataloader(\n",
    "            samples, biencoder.tokenizer, biencoder_params\n",
    "        )\n",
    "\n",
    "        # run biencoder\n",
    "        if logger:\n",
    "            logger.info(\"run biencoder\")\n",
    "        top_k = args.top_k\n",
    "        labels, nns, scores, encodings = _run_biencoder(\n",
    "            biencoder, dataloader, candidate_encoding, top_k, faiss_indexer, bool(args.save_encodings) if hasattr(args, 'save_encodings') else False\n",
    "        )\n",
    "\n",
    "        if hasattr(args, 'save_encodings') and args.save_encodings:\n",
    "            with open(args.save_encodings, 'w') as fd:\n",
    "                for _enc, _lab in zip(encodings, labels):\n",
    "                    assert len(_lab) == 1\n",
    "                    _lab = int(_lab[0])\n",
    "                    current = {\n",
    "                        \"encoding\": _enc,\n",
    "                        \"label\": _lab,\n",
    "                        \"wikipedia_id\": 0 if local_id2wikipedia_id is None else local_id2wikipedia_id[_lab],\n",
    "                        \"title\": id2title[_lab]\n",
    "                    }\n",
    "                    json.dump(current, fd)\n",
    "                    fd.write('\\n')\n",
    "\n",
    "        if args.save_scores_bi:\n",
    "            scores_bi = {\n",
    "                \"labels\": [l.tolist() for l in labels],\n",
    "                \"scores\": [l.tolist() for l in scores],\n",
    "                \"nns\": [l.tolist() for l in nns]\n",
    "            }\n",
    "            with open(args.save_scores_bi, 'w') as fd:\n",
    "                json.dump(scores_bi, fd)\n",
    "                \n",
    "        if hasattr(args, 'nil_prediction') and args.nil_prediction and nil_prediction_model:\n",
    "            global nil_features_bi\n",
    "            global nil_preds_bi\n",
    "\n",
    "            nil_features_bi = np.array(list(map(lambda x: list(_scores_get_stats(x).values()), scores)))\n",
    "            nil_features_bi = pd.DataFrame(data=nil_features_bi, columns=['max_bi',\n",
    "                       'second_bi',\n",
    "                       'min_bi',\n",
    "                       'mean_bi',\n",
    "                       'median_bi',\n",
    "                       'stdev_bi'])\n",
    "            nil_features_bi = nil_features_bi[nil_prediction_features_bi]\n",
    "\n",
    "            #nil_features = nil_prediction_scaler.transform(nil_features)\n",
    "            ##nil_preds = nil_prediction_model.predict(nil_features)\n",
    "            #nil_features = torch.FloatTensor(nil_features)\n",
    "            #nil_preds = nil_prediction_model(nil_features)\n",
    "            #nil_preds = torch.sigmoid(nil_preds)\n",
    "            #nil_preds = nil_preds.cpu().detach().numpy().reshape(-1,).tolist()\n",
    "            \n",
    "            nil_preds_bi = nil_prediction_model_bi.predict_proba(nil_features_bi)\n",
    "            nil_preds_bi = np.array([i_1 for _, i_1 in nil_preds_bi])\n",
    "\n",
    "\n",
    "        if args.interactive:\n",
    "\n",
    "            print(\"\\nfast (biencoder) predictions:\")\n",
    "\n",
    "            _print_colorful_text(text, samples)\n",
    "\n",
    "            # print biencoder prediction\n",
    "            idx = 0\n",
    "            for entity_list, sample, _score, nil_p in zip(nns, samples, scores, nil_preds_bi):\n",
    "                e_id = entity_list[0]\n",
    "                e_title = id2title[e_id]\n",
    "                e_text = id2text[e_id]\n",
    "                e_url = id2url[e_id]\n",
    "                _print_colorful_prediction(\n",
    "                    idx, sample, e_id, e_title, e_text, e_url, args.show_url\n",
    "                )\n",
    "                print(\"bi_Score:\", _score[0])\n",
    "                print(\"all scores:\", _score[1:])\n",
    "                print('NIL:', nil_p)\n",
    "                idx += 1\n",
    "            print()\n",
    "\n",
    "            if args.fast:\n",
    "                # use only biencoder\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            biencoder_accuracy = -1\n",
    "            recall_at = -1\n",
    "            if not keep_all:\n",
    "                # get recall values\n",
    "                top_k = args.top_k\n",
    "                x = []\n",
    "                y = []\n",
    "                for i in range(1, top_k):\n",
    "                    temp_y = 0.0\n",
    "                    for label, top in zip(labels, nns):\n",
    "                        if label in top[:i]:\n",
    "                            temp_y += 1\n",
    "                    if len(labels) > 0:\n",
    "                        temp_y /= len(labels)\n",
    "                    x.append(i)\n",
    "                    y.append(temp_y)\n",
    "                # plt.plot(x, y)\n",
    "                biencoder_accuracy = y[0]\n",
    "                recall_at = y[-1]\n",
    "                print(\"biencoder accuracy: %.4f\" % biencoder_accuracy)\n",
    "                print(\"biencoder recall@%d: %.4f\" % (top_k, y[-1]))\n",
    "\n",
    "            if args.fast:\n",
    "\n",
    "                predictions = []\n",
    "                for entity_list in nns:\n",
    "                    sample_prediction = []\n",
    "                    for e_id in entity_list:\n",
    "                        e_title = id2title[e_id]\n",
    "                        sample_prediction.append(e_title)\n",
    "                    predictions.append(sample_prediction)\n",
    "\n",
    "                # use only biencoder\n",
    "                return (\n",
    "                    biencoder_accuracy,\n",
    "                    recall_at,\n",
    "                    -1,\n",
    "                    -1,\n",
    "                    len(samples),\n",
    "                    predictions,\n",
    "                    scores,\n",
    "                )\n",
    "\n",
    "        # prepare crossencoder data\n",
    "        context_input, candidate_input, label_input = prepare_crossencoder_data(\n",
    "            crossencoder.tokenizer, samples, labels, nns, id2title, id2text, keep_all,\n",
    "        )\n",
    "\n",
    "        context_input = modify(\n",
    "            context_input, candidate_input, crossencoder_params[\"max_seq_length\"]\n",
    "        )\n",
    "\n",
    "        dataloader = _process_crossencoder_dataloader(\n",
    "            context_input, label_input, crossencoder_params\n",
    "        )\n",
    "\n",
    "        # run crossencoder and get accuracy\n",
    "        accuracy, index_array, unsorted_scores = _run_crossencoder(\n",
    "            crossencoder,\n",
    "            dataloader,\n",
    "            logger,\n",
    "            context_len=biencoder_params[\"max_context_length\"],\n",
    "        )\n",
    "        if hasattr(args, 'nil_prediction') and args.nil_prediction and nil_prediction_model:\n",
    "            global nil_features\n",
    "            global nil_preds\n",
    "\n",
    "            nil_features = np.array(list(\n",
    "                map(\n",
    "                    lambda x: list(x[0].values()) + list(x[1].values()),\n",
    "                    #lambda x: list(x[0].values()),\n",
    "                    list(\n",
    "                        zip(\n",
    "                            list(map(_scores_get_stats, scores)), # bi scores\n",
    "                            list(map(_scores_get_stats, unsorted_scores)) # cross scores\n",
    "                        )))))\n",
    "            nil_features = pd.DataFrame(data=nil_features, columns=['max_bi',\n",
    "                       'second_bi',\n",
    "                       'min_bi',\n",
    "                       'mean_bi',\n",
    "                       'median_bi',\n",
    "                       'stdev_bi',\n",
    "                       'max_cross',\n",
    "                       'second_cross',\n",
    "                       'min_cross',\n",
    "                       'mean_cross',\n",
    "                       'median_cross',\n",
    "                       'stdev_cross'])\n",
    "            nil_features = nil_features[nil_prediction_features]\n",
    "\n",
    "            #nil_features = nil_prediction_scaler.transform(nil_features)\n",
    "            ##nil_preds = nil_prediction_model.predict(nil_features)\n",
    "            #nil_features = torch.FloatTensor(nil_features)\n",
    "            #nil_preds = nil_prediction_model(nil_features)\n",
    "            #nil_preds = torch.sigmoid(nil_preds)\n",
    "            #nil_preds = nil_preds.cpu().detach().numpy().reshape(-1,).tolist()\n",
    "            \n",
    "            nil_preds = nil_prediction_model.predict_proba(nil_features)\n",
    "            nil_preds = np.array([i_1 for _, i_1 in nil_preds])\n",
    "\n",
    "        if args.save_scores_cross:\n",
    "            print('----- Score cross length -----')\n",
    "            print('labels', len(labels))\n",
    "            print('unsorted_scores', len(unsorted_scores))\n",
    "            print('index_array', len(index_array))\n",
    "            print('nns', len(nns))\n",
    "            scores_cross = {\n",
    "                \"labels\": [l.tolist() for l in labels],\n",
    "                \"unsorted_scores\": [l.tolist() for l in unsorted_scores],\n",
    "                \"index_array\": index_array.tolist(),\n",
    "                \"nns\": [l.tolist() for l in nns]\n",
    "            }\n",
    "            with open(args.save_scores_cross, 'w') as fd:\n",
    "                json.dump(scores_cross, fd)\n",
    "\n",
    "        if args.interactive:\n",
    "\n",
    "            print(\"\\naccurate (crossencoder) predictions:\")\n",
    "\n",
    "            _print_colorful_text(text, samples)\n",
    "\n",
    "            # print crossencoder prediction\n",
    "            idx = 0\n",
    "            for entity_list, index_list, sample, _scores, _nil in zip(nns, index_array, samples, unsorted_scores, nil_preds):\n",
    "                e_id = entity_list[index_list[-1]]\n",
    "                e_title = id2title[e_id]\n",
    "                e_text = id2text[e_id]\n",
    "                e_url = id2url[e_id]\n",
    "                _print_colorful_prediction(\n",
    "                    idx, sample, e_id, e_title, e_text, e_url, args.show_url\n",
    "                )\n",
    "                print(\"cross_score:\", _scores[index_list[-1]])\n",
    "                print(\"all scores:\", _scores)\n",
    "                print(\"NIL score:\", _nil)\n",
    "                idx += 1\n",
    "            print()\n",
    "        else:\n",
    "\n",
    "            scores = []\n",
    "            predictions = []\n",
    "            for entity_list, index_list, scores_list in zip(\n",
    "                nns, index_array, unsorted_scores\n",
    "            ):\n",
    "\n",
    "                index_list = index_list.tolist()\n",
    "\n",
    "                # descending order\n",
    "                index_list.reverse()\n",
    "\n",
    "                sample_prediction = []\n",
    "                sample_scores = []\n",
    "                for index in index_list:\n",
    "                    e_id = entity_list[index]\n",
    "                    e_title = id2title[e_id]\n",
    "                    sample_prediction.append(e_title)\n",
    "                    sample_scores.append(scores_list[index])\n",
    "                predictions.append(sample_prediction)\n",
    "                scores.append(sample_scores)\n",
    "\n",
    "            crossencoder_normalized_accuracy = -1\n",
    "            overall_unormalized_accuracy = -1\n",
    "            if not keep_all:\n",
    "                crossencoder_normalized_accuracy = accuracy\n",
    "                print(\n",
    "                    \"crossencoder normalized accuracy: %.4f\"\n",
    "                    % crossencoder_normalized_accuracy\n",
    "                )\n",
    "\n",
    "                if len(samples) > 0:\n",
    "                    overall_unormalized_accuracy = (\n",
    "                        crossencoder_normalized_accuracy * len(label_input) / len(samples)\n",
    "                    )\n",
    "                print(\n",
    "                    \"overall unnormalized accuracy: %.4f\" % overall_unormalized_accuracy\n",
    "                )\n",
    "            return (\n",
    "                biencoder_accuracy,\n",
    "                recall_at,\n",
    "                crossencoder_normalized_accuracy,\n",
    "                overall_unormalized_accuracy,\n",
    "                len(samples),\n",
    "                predictions,\n",
    "                scores,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce6c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive\n",
    "args = Dict()\n",
    "\n",
    "args.nil_prediction = True\n",
    "#args.nil_prediction_scaler = \"../models/nil_pred/indexer/stdscaler+max_bi+second_bi+min_bi+mean_bi+median_bi+stdev_bi+max_cross+second_cross+min_cross+mean_cross+median_cross+stdev_cross+train_hard_aug.pkl\"\n",
    "#args.nil_prediction_model = \"../models/nil_pred/indexer/nrl+max_bi+second_bi+min_bi+mean_bi+median_bi+stdev_bi+max_cross+second_cross+min_cross+mean_cross+median_cross+stdev_cross+train_hard_aug.torch\"\n",
    "args.nil_prediction_model_bi = \"../models/nil_pred/models_ip/svc_bi+train_hard_aug.pkl\"\n",
    "args.nil_prediction_features_bi = ['max_bi', 'second_bi', 'min_bi', 'mean_bi', 'median_bi' ,'stdev_bi']\n",
    "args.nil_prediction_model = \"../models/nil_pred/models_ip/svc_all+train_hard_aug.pkl\"\n",
    "args.nil_prediction_features = ['max_bi', 'second_bi', 'min_bi', 'mean_bi', 'median_bi' ,'stdev_bi', 'max_cross', 'second_cross', 'min_cross', 'mean_cross', 'median_cross', 'stdev_cross']\n",
    "args.interactive = True\n",
    "args.top_k = 10\n",
    "args.biencoder_config = \"../models/biencoder_wiki_large.json\"\n",
    "args.biencoder_model = \"../models/biencoder_wiki_large.bin\"\n",
    "args.crossencoder_config = \"../models/crossencoder_wiki_large.json\"\n",
    "args.crossencoder_model = \"../models/crossencoder_wiki_large.bin\"\n",
    "args.entity_catalogue = \"../models/entity.jsonl\"\n",
    "args.entity_encoding = \"../models/all_entities_large.t7\"\n",
    "#bi_higher_is_better = False\n",
    "#args.faiss_index = \"hnsw\"\n",
    "#args.index_path = \"../models/faiss_hnsw_index.pkl\"\n",
    "bi_higher_is_better = True\n",
    "args.faiss_index = \"flat\"\n",
    "args.index_path = \"../models/faiss_flat_index.pkl\"\n",
    "\n",
    "logger = utils.get_logger(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test file\n",
    "args = Dict()\n",
    "\n",
    "args.nil_prediction = False\n",
    "\n",
    "args.interactive = False\n",
    "args.top_k = 10\n",
    "args.biencoder_config = \"../models/biencoder_wiki_large.json\"\n",
    "args.biencoder_model = \"../models/biencoder_wiki_large.bin\"\n",
    "args.crossencoder_config = \"../models/crossencoder_wiki_large.json\"\n",
    "args.crossencoder_model = \"../models/crossencoder_wiki_large.bin\"\n",
    "args.entity_catalogue = \"../models/entity.jsonl\"\n",
    "args.entity_encoding = \"../models/all_entities_large.t7\"\n",
    "#args.faiss_index = \"hnsw\"\n",
    "#args.index_path = \"../models/faiss_hnsw_index.pkl\"\n",
    "args.faiss_index = None\n",
    "args.index_path = None\n",
    "\n",
    "args.test_mentions = \"../data/BLINK_benchmark/ace2004_questions.jsonl\"\n",
    "\n",
    "args.save_scores_bi = \"../output/scores_bi.json\"\n",
    "args.save_scores_cross = \"../output/scores_cross.json\"\n",
    "\n",
    "args.keep_all = True\n",
    "args.consider_all = True\n",
    "\n",
    "logger = utils.get_logger(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get encodings (starting from jsonl file)\n",
    "args = Dict()\n",
    "\n",
    "args.interactive = False\n",
    "args.top_k = 10\n",
    "args.biencoder_config = \"../models/biencoder_wiki_large.json\"\n",
    "args.biencoder_model = \"../models/biencoder_wiki_large.bin\"\n",
    "args.crossencoder_config = \"../models/crossencoder_wiki_large.json\"\n",
    "args.crossencoder_model = \"../models/crossencoder_wiki_large.bin\"\n",
    "args.entity_catalogue = \"../models/entity.jsonl\"\n",
    "args.entity_encoding = \"../models/all_entities_large.t7\"\n",
    "args.faiss_index = \"hnsw\"\n",
    "args.index_path = \"../models/faiss_hnsw_index.pkl\"\n",
    "\n",
    "#args.test_mentions = \"../data/test10.jsonl\"\n",
    "args.test_mentions = \"../data/BLINK_benchmark/AIDA-YAGO2_train.jsonl\"\n",
    "\n",
    "args.save_encodings = \"../output/encodings/AIDA-YAGO2_train_encodings.jsonl\"\n",
    "\n",
    "logger = utils.get_logger(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bdb846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not rerun\n",
    "# takes time and memory\n",
    "models = load_models(args, logger)\n",
    "print(\"Models load complete.\")\n",
    "local_id2wikipedia_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_local_id = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d055d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "biencoder = models[0]\n",
    "biencoder_params = models[1]\n",
    "crossencoder = models[2]\n",
    "crossencoder_params = models[3]\n",
    "candidate_encoding = models[4]\n",
    "title2id = models[5]\n",
    "id2title = models[6]\n",
    "id2text = models[7]\n",
    "wikipedia_id2local_id = models[8]\n",
    "faiss_indexer = models[9]\n",
    "if (hasattr(args, 'save_encodings') and args.save_encodings) or _get_local_id:\n",
    "    local_id2wikipedia_id = {}\n",
    "    for k,v in wikipedia_id2local_id.items():\n",
    "        local_id2wikipedia_id[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to define run function above\n",
    "run(args, logger, *models, local_id2wikipedia_id=local_id2wikipedia_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62298be6",
   "metadata": {},
   "source": [
    "# bi context encoding tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = NER.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae06507",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Henry, king of England from 22 April 1509, married for the fifth time with Catherine Howard in 1540.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209365da",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = _annotate(ner_model, [text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc01c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_dataloader = _process_biencoder_dataloader(\n",
    "            samples, biencoder.tokenizer, biencoder_params\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14822836",
   "metadata": {},
   "outputs": [],
   "source": [
    "_context_encoding = None\n",
    "biencoder.model.eval()\n",
    "for batch in bi_dataloader:\n",
    "    context_input, _, label_ids = batch\n",
    "    with torch.no_grad():\n",
    "        context_encoding = biencoder.encode_context(context_input).numpy()\n",
    "        context_encoding = np.ascontiguousarray(context_encoding)\n",
    "        _context_encoding = context_encoding if _context_encoding is None else np.concatenate([_context_encoding, context_encoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e88427",
   "metadata": {},
   "outputs": [],
   "source": [
    "_context_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "retrieved = faiss_indexer.search_knn(_context_encoding, 10)\n",
    "\n",
    "print('seconds: ', time.monotonic() - start_time)\n",
    "retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[id2title[i] for i in retrieved[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc5a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2text[4679300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco = faiss_indexer.index.reconstruct(4679300)\n",
    "reco.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1131f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "_context_encoding[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44976e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "(_context_encoding[0] ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array([1,2,3])**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(reco[:1024], _context_encoding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0629b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = reco[:1024]\n",
    "b = _context_encoding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d826a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(((a-b)**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe695b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(reco, reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78283235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original = b\n",
    "original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(b, original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7806b",
   "metadata": {},
   "source": [
    "# cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24dba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blink.crossencoder.data_process import prepare_crossencoder_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af99834",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_input_list = prepare_crossencoder_mentions(crossencoder.tokenizer, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43961d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_input = torch.LongTensor(context_input_list)\n",
    "context_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_input = torch.reshape(context_input, (context_input.shape[0], 1, -1))\n",
    "context_input = modify(\n",
    "    context_input, candidate_input, crossencoder_params[\"max_seq_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_input_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = args.top_k\n",
    "labels, nns, scores, encodings = _run_biencoder(\n",
    "    biencoder, bi_dataloader, candidate_encoding, top_k, faiss_indexer, False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846247be",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2text[3422691]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare crossencoder data\n",
    "context_input, candidate_input, label_input = prepare_crossencoder_data(\n",
    "    crossencoder.tokenizer, samples, labels, nns, id2title, id2text, keep_all=True,\n",
    ")\n",
    "\n",
    "context_input = modify(\n",
    "    context_input, candidate_input, crossencoder_params[\"max_seq_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408984df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_dataloader = _process_crossencoder_dataloader(\n",
    "    context_input, label_input, crossencoder_params\n",
    ")\n",
    "\n",
    "crossencoder.model.eval()\n",
    "device = \"cuda\"\n",
    "crossencoder.to(device)\n",
    "for batch in cross_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        crossencoder(batch[0], batch[1], biencoder_params[\"max_context_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fbaf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossencoder.model.eval()\n",
    "with torch.no_grad():\n",
    "    res = crossencoder(context_input.to(\"cuda\"), label_input.to(\"cuda\"), biencoder_params[\"max_context_length\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c85cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[1][1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9fd4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_dataloader = _process_crossencoder_dataloader(\n",
    "    context_input, label_input, crossencoder_params\n",
    ")\n",
    "\n",
    "# run crossencoder and get accuracy\n",
    "accuracy, index_array, unsorted_scores = _run_crossencoder(\n",
    "    crossencoder,\n",
    "    cross_dataloader,\n",
    "    logger,\n",
    "    context_len=biencoder_params[\"max_context_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7cb1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2url = {\n",
    "    v: \"https://en.wikipedia.org/wiki?curid=%s\" % k\n",
    "    for k, v in wikipedia_id2local_id.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23cbc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\naccurate (crossencoder) predictions:\")\n",
    "\n",
    "_print_colorful_text(text, samples)\n",
    "\n",
    "# print crossencoder prediction\n",
    "idx = 0\n",
    "for entity_list, index_list, sample, _scores in zip(nns, index_array, samples, unsorted_scores):\n",
    "    e_id = entity_list[index_list[-1]]\n",
    "    e_title = id2title[e_id]\n",
    "    e_text = id2text[e_id]\n",
    "    e_url = id2url[e_id]\n",
    "    _print_colorful_prediction(\n",
    "        idx, sample, e_id, e_title, e_text, e_url, args.show_url\n",
    "    )\n",
    "    print(\"cross_score:\", _scores[index_list[-1]])\n",
    "    print(\"all scores:\", _scores)\n",
    "    idx += 1\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "nns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302df6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in cross_dataloader:\n",
    "    print(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eedef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771c8d6",
   "metadata": {},
   "source": [
    "## save entity ids structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entity_ids/title2id.pickle', 'wb') as fd:\n",
    "    pickle.dump(title2id, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7765d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entity_ids/id2title.pickle', 'wb') as fd:\n",
    "    pickle.dump(id2title, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entity_ids/id2text.pickle', 'wb') as fd:\n",
    "    pickle.dump(id2text, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entity_ids/wikipedia_id2local_id.pickle', 'wb') as fd:\n",
    "    pickle.dump(wikipedia_id2local_id, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entity_ids/local_id2wikipedia_id.pickle', 'wb') as fd:\n",
    "    pickle.dump(local_id2wikipedia_id, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b06c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2title[3674818]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_id2local_id[243710]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ab964",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_id2wikipedia_id[3674818]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ffc4f3",
   "metadata": {},
   "source": [
    "# get encodings fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab31430",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.save_encodings = '../output/AIDA-YAGO2_train_encodings.jsonl'\n",
    "args.test_mentions = '../data/BLINK_benchmark/AIDA-YAGO2_train.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b710c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = _get_test_samples(\n",
    "                    args.test_mentions,\n",
    "                    args.test_entities,\n",
    "                    title2id,\n",
    "                    wikipedia_id2local_id,\n",
    "                    logger,\n",
    "                    consider_all= True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_biencoder_only_encodings(biencoder, dataloader, candidate_encoding, top_k=100, indexer=None, save_encodings=True):\n",
    "    biencoder.model.eval()\n",
    "    labels = []\n",
    "    #nns = []\n",
    "    #all_scores = []\n",
    "    encodings = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        context_input, _, label_ids = batch\n",
    "        with torch.no_grad():\n",
    "            if indexer is not None:\n",
    "                context_encoding = biencoder.encode_context(context_input).numpy()\n",
    "                context_encoding = np.ascontiguousarray(context_encoding)\n",
    "                if save_encodings:\n",
    "                    encodings.extend([e.tolist() for e in context_encoding])\n",
    "                #scores, indicies = indexer.search_knn(context_encoding, top_k)\n",
    "            else:\n",
    "                raise Exception('not implemented for only getting encodings.')\n",
    "\n",
    "        labels.extend(label_ids.data.numpy())\n",
    "    return labels, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = _process_biencoder_dataloader(\n",
    "    samples, biencoder.tokenizer, biencoder_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b02aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run biencoder\n",
    "\n",
    "top_k = args.top_k\n",
    "labels, encodings = _run_biencoder_only_encodings(\n",
    "    biencoder, dataloader, candidate_encoding, top_k, faiss_indexer, bool(args.save_encodings) if hasattr(args, 'save_encodings') else False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.save_encodings, 'w') as fd:\n",
    "    for _enc, _lab in zip(encodings, labels):\n",
    "        assert len(_lab) == 1\n",
    "        _lab = int(_lab[0])\n",
    "        current = {\n",
    "            \"encoding\": _enc,\n",
    "            \"label\": _lab,\n",
    "            \"wikipedia_id\": 0 if local_id2wikipedia_id is None or _lab not in local_id2wikipedia_id else local_id2wikipedia_id[_lab],\n",
    "            \"title\": id2title[_lab] if _lab in id2title else \"**NOTFOUND**\"\n",
    "        }\n",
    "        json.dump(current, fd)\n",
    "        fd.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6117548",
   "metadata": {},
   "source": [
    "# encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l ../output/*.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blink.indexer.faiss_indexer import DenseFlatIndexer\n",
    "from sklearn_extra.cluster import KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d65d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "GetMedoid = lambda vX: KMedoids(n_clusters=1).fit(np.stack(vX)).cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb689d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_p = ['../output/AIDA-YAGO2_testa_encodings.jsonl',\n",
    " '../output/AIDA-YAGO2_testb_encodings.jsonl',\n",
    " '../output/AIDA-YAGO2_train_encodings.jsonl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('../output/AIDA-YAGO2_train_encodings.jsonl', lines=True)\n",
    "testa_df = pd.read_json('../output/AIDA-YAGO2_testa_encodings.jsonl', lines=True)\n",
    "testb_df = pd.read_json('../output/AIDA-YAGO2_testb_encodings.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first\n",
    "to_index = pd.DataFrame(train_df.query('wikipedia_id > 0').groupby('wikipedia_id')['encoding'].first())\n",
    "to_index = to_index.sample(frac=1) # shuffle\n",
    "to_index['index'] = range(to_index.shape[0])\n",
    "to_index['wikipedia_id'] = to_index.index\n",
    "to_index = to_index.set_index('index')\n",
    "to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# medoid\n",
    "to_index = pd.DataFrame(train_df.query('wikipedia_id > 0').groupby('wikipedia_id')['encoding'].apply(\n",
    "    lambda x: GetMedoid(x)[0]))\n",
    "to_index = to_index.sample(frac=1) # shuffle\n",
    "to_index['index'] = range(to_index.shape[0])\n",
    "to_index['wikipedia_id'] = to_index.index\n",
    "to_index = to_index.set_index('index')\n",
    "to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index all (same entity multiple times)\n",
    "to_index = pd.DataFrame(train_df.query('wikipedia_id > 0')[['wikipedia_id', 'encoding']])\n",
    "to_index = to_index.sample(frac=1) # shuffle\n",
    "to_index['index'] = range(to_index.shape[0])\n",
    "to_index = to_index.set_index('index')\n",
    "to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88373566",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_1 = DenseFlatIndexer(1024, 50000) # 1024 dimensions, 50000 default as BLINK\n",
    "index_1.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index in batch of 100 mentions\n",
    "for i in range(100, to_index.shape[0], 100):\n",
    "    #print(i-100, i)\n",
    "    index_1.index_data(\n",
    "        np.stack(\n",
    "            to_index.iloc[i-100:i]['encoding'].values\n",
    "        ).astype('float32'))\n",
    "\n",
    "# index last batch\n",
    "index_1.index_data(\n",
    "    np.stack(\n",
    "        to_index.iloc[i:to_index.shape[0]]['encoding'].values\n",
    "    ).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4202ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert index_1.index.ntotal == to_index.shape[0]\n",
    "index_1.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46120cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "testa_linking_results = index_1.search_knn(np.stack(testa_df['encoding'].values).astype('float32'), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dae19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfun = np.vectorize(lambda x: to_index.iloc[x]['wikipedia_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f04383",
   "metadata": {},
   "outputs": [],
   "source": [
    "testa_linking_results_wiki_id = myfun(testa_linking_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ec24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_isin(x, array):\n",
    "    array =  array.tolist()\n",
    "    if x in array:\n",
    "        return array.index(x)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128706a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_testa = pd.DataFrame(testa_df.apply(lambda x: _eval_isin(x['wikipedia_id'], testa_linking_results_wiki_id[x.name]), axis=1), columns=['found_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32101250",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_testa.dropna().shape[0]/eval_testa.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval only on entities that are in the index\n",
    "eval_testa['wikipedia_id'] = testa_df['wikipedia_id']\n",
    "eval_testa_filtered = eval_testa[eval_testa['wikipedia_id'].isin(to_index['wikipedia_id'])]\n",
    "eval_testa_filtered.shape[0]/eval_testa.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(test_df, name):\n",
    "    eval_df = pd.DataFrame(data=[\n",
    "         name,\n",
    "         test_df.dropna().query('found_at < 1').shape[0]/test_df.shape[0],\n",
    "         test_df.dropna().query('found_at < 2').shape[0]/test_df.shape[0],\n",
    "         test_df.dropna().query('found_at < 3').shape[0]/test_df.shape[0],\n",
    "         test_df.dropna().query('found_at < 5').shape[0]/test_df.shape[0],\n",
    "         test_df.dropna().query('found_at < 10').shape[0]/test_df.shape[0],\n",
    "         test_df.dropna().query('found_at < 30').shape[0]/test_df.shape[0],\n",
    "         test_df.dropna().query('found_at < 100').shape[0]/test_df.shape[0],\n",
    "\n",
    "    ], index = [\n",
    "        'name',\n",
    "        'recall@1',\n",
    "        'recall@2',\n",
    "        'recall@3',\n",
    "        'recall@5',\n",
    "        'recall@10',\n",
    "        'recall@30',\n",
    "        'recall@100',\n",
    "    ])\n",
    "    print(eval_df.to_markdown())\n",
    "    print()\n",
    "    print(eval_df.to_latex())\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b51dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test(eval_testa_filtered, 'test a index all brutally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testb_linking_results = index_1.search_knn(np.stack(testb_df['encoding'].values).astype('float32'), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d4eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "testb_linking_results_wiki_id = myfun(testb_linking_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_testb = pd.DataFrame(testb_df.apply(lambda x: _eval_isin(x['wikipedia_id'], testb_linking_results_wiki_id[x.name]), axis=1), columns=['found_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bcf712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval only on entities that are in the index\n",
    "eval_testb['wikipedia_id'] = testb_df['wikipedia_id']\n",
    "eval_testb_filtered = eval_testb[eval_testb['wikipedia_id'].isin(to_index['wikipedia_id'])]\n",
    "eval_testb_filtered.shape[0]/eval_testb.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888fda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test(eval_testb_filtered, 'test b index all brutally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fa531",
   "metadata": {},
   "outputs": [],
   "source": [
    "(to_index['wikipedia_id'].value_counts() > 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_testb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292ebed",
   "metadata": {},
   "source": [
    "|            | 0                  |\n",
    "|:-----------|:-------------------|\n",
    "| name       | test a first       |\n",
    "| recall@1   | 0.7374071015689513 |\n",
    "| recall@2   | 0.8219102669969722 |\n",
    "| recall@3   | 0.8560418387007982 |\n",
    "| recall@5   | 0.8882466281310212 |\n",
    "| recall@10  | 0.9193503991191853 |\n",
    "| recall@30  | 0.9581612992017616 |\n",
    "| recall@100 | 0.985962014863749  |\n",
    "\n",
    "\\begin{tabular}{ll}\n",
    "\\toprule\n",
    "{} &             0 \\\\\n",
    "\\midrule\n",
    "name       &  test a first \\\\\n",
    "recall@1   &      0.737407 \\\\\n",
    "recall@2   &       0.82191 \\\\\n",
    "recall@3   &      0.856042 \\\\\n",
    "recall@5   &      0.888247 \\\\\n",
    "recall@10  &       0.91935 \\\\\n",
    "recall@30  &      0.958161 \\\\\n",
    "recall@100 &      0.985962 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4535a30",
   "metadata": {},
   "source": [
    "|            | 0                  |\n",
    "|:-----------|:-------------------|\n",
    "| name       | test b first       |\n",
    "| recall@1   | 0.7006125574272588 |\n",
    "| recall@2   | 0.7974732006125574 |\n",
    "| recall@3   | 0.832312404287902  |\n",
    "| recall@5   | 0.8709800918836141 |\n",
    "| recall@10  | 0.9046707503828484 |\n",
    "| recall@30  | 0.9525267993874426 |\n",
    "| recall@100 | 0.9820061255742726 |\n",
    "\n",
    "\\begin{tabular}{ll}\n",
    "\\toprule\n",
    "{} &             0 \\\\\n",
    "\\midrule\n",
    "name       &  test b first \\\\\n",
    "recall@1   &      0.700613 \\\\\n",
    "recall@2   &      0.797473 \\\\\n",
    "recall@3   &      0.832312 \\\\\n",
    "recall@5   &       0.87098 \\\\\n",
    "recall@10  &      0.904671 \\\\\n",
    "recall@30  &      0.952527 \\\\\n",
    "recall@100 &      0.982006 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ca9f3",
   "metadata": {},
   "source": [
    "|            | 0                  |\n",
    "|:-----------|:-------------------|\n",
    "| name       | test a medoid      |\n",
    "| recall@1   | 0.7946600605560143 |\n",
    "| recall@2   | 0.8769611890999174 |\n",
    "| recall@3   | 0.910542251582714  |\n",
    "| recall@5   | 0.9380677126341866 |\n",
    "| recall@10  | 0.9587118084227911 |\n",
    "| recall@30  | 0.9870630333058079 |\n",
    "| recall@100 | 0.9972474538948527 |\n",
    "\n",
    "\\begin{tabular}{ll}\n",
    "\\toprule\n",
    "{} &              0 \\\\\n",
    "\\midrule\n",
    "name       &  test a medoid \\\\\n",
    "recall@1   &        0.79466 \\\\\n",
    "recall@2   &       0.876961 \\\\\n",
    "recall@3   &       0.910542 \\\\\n",
    "recall@5   &       0.938068 \\\\\n",
    "recall@10  &       0.958712 \\\\\n",
    "recall@30  &       0.987063 \\\\\n",
    "recall@100 &       0.997247 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4beb8e",
   "metadata": {},
   "source": [
    "|            | 0                  |\n",
    "|:-----------|:-------------------|\n",
    "| name       | test b medoid      |\n",
    "| recall@1   | 0.7687595712098009 |\n",
    "| recall@2   | 0.8503062787136294 |\n",
    "| recall@3   | 0.8862940275650842 |\n",
    "| recall@5   | 0.9142419601837672 |\n",
    "| recall@10  | 0.9498468606431854 |\n",
    "| recall@30  | 0.9900459418070444 |\n",
    "| recall@100 | 0.9973200612557427 |\n",
    "\n",
    "\\begin{tabular}{ll}\n",
    "\\toprule\n",
    "{} &              0 \\\\\n",
    "\\midrule\n",
    "name       &  test b medoid \\\\\n",
    "recall@1   &        0.76876 \\\\\n",
    "recall@2   &       0.850306 \\\\\n",
    "recall@3   &       0.886294 \\\\\n",
    "recall@5   &       0.914242 \\\\\n",
    "recall@10  &       0.949847 \\\\\n",
    "recall@30  &       0.990046 \\\\\n",
    "recall@100 &        0.99732 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7864e53",
   "metadata": {},
   "source": [
    "|            | 0                         |\n",
    "|:-----------|:--------------------------|\n",
    "| name       | test a index all brutally |\n",
    "| recall@1   | 0.9388934764657308        |\n",
    "| recall@2   | 0.9584365538122763        |\n",
    "| recall@3   | 0.9666941921277181        |\n",
    "| recall@5   | 0.9774291219377924        |\n",
    "| recall@10  | 0.9854115056427195        |\n",
    "| recall@30  | 0.9931186347371318        |\n",
    "| recall@100 | 0.9958711808422791        |\n",
    "\n",
    "\\begin{tabular}{ll}\n",
    "\\toprule\n",
    "{} &                          0 \\\\\n",
    "\\midrule\n",
    "name       &  test a index all brutally \\\\\n",
    "recall@1   &                   0.938893 \\\\\n",
    "recall@2   &                   0.958437 \\\\\n",
    "recall@3   &                   0.966694 \\\\\n",
    "recall@5   &                   0.977429 \\\\\n",
    "recall@10  &                   0.985412 \\\\\n",
    "recall@30  &                   0.993119 \\\\\n",
    "recall@100 &                   0.995871 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158ae51",
   "metadata": {},
   "source": [
    "|            | 0                         |\n",
    "|:-----------|:--------------------------|\n",
    "| name       | test b index all brutally |\n",
    "| recall@1   | 0.9291730474732006        |\n",
    "| recall@2   | 0.9555895865237366        |\n",
    "| recall@3   | 0.9655436447166922        |\n",
    "| recall@5   | 0.9701378254211332        |\n",
    "| recall@10  | 0.9785604900459418        |\n",
    "| recall@30  | 0.9896630934150077        |\n",
    "| recall@100 | 0.9950229709035222        |\n",
    "\n",
    "\\begin{tabular}{ll}\n",
    "\\toprule\n",
    "{} &                          0 \\\\\n",
    "\\midrule\n",
    "name       &  test b index all brutally \\\\\n",
    "recall@1   &                   0.929173 \\\\\n",
    "recall@2   &                    0.95559 \\\\\n",
    "recall@3   &                   0.965544 \\\\\n",
    "recall@5   &                   0.970138 \\\\\n",
    "recall@10  &                    0.97856 \\\\\n",
    "recall@30  &                   0.989663 \\\\\n",
    "recall@100 &                   0.995023 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "over5 = pd.DataFrame(train_df.query('wikipedia_id != 0')['wikipedia_id'].value_counts()).query('wikipedia_id > 5').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_testa['wikipedia_id'] = testa_df['wikipedia_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_testa[eval_testa['wikipedia_id'].isin(to_index['wikipedia_id'])].shape[0]/eval_testa.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f821cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_count_train_testa = pd.DataFrame(train_df.query('wikipedia_id != 0')['wikipedia_id'].value_counts()).join(\n",
    "    pd.DataFrame(testa_df.query('wikipedia_id != 0')['wikipedia_id'].value_counts()),\n",
    "    how='inner', lsuffix='train', rsuffix='_testa')\n",
    "wiki_count_train_testa['min_count'] = wiki_count_train_testa.min(axis=1)\n",
    "wiki_count_train_testa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_count_train_testa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dee976",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test(eval_testa[eval_testa['wikipedia_id'].isin(\n",
    "    wiki_count_train_testa.query('wikipedia_idtrain < 5 and wikipedia_id_testa > 5').index\n",
    ")], 'aaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "testa_linking_results_wiki_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "testa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testa_df.loc[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e0f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5a0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f51d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001bb883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d135b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encodings_df['encoding'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_df['encoding'] = encodings_df['encoding'].apply(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfe041",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_df.iloc[0:1000].query('wikipedia_id > 0').groupby('wikipedia_id')['encoding'].first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_df.query('wikipedia_id == 17867')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822cecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = encodings_df.groupby('wikipedia_id')['encoding'].first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = encodings_df.groupby('wikipedia_id')['encoding'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb938d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = mean.sample(frac=1)\n",
    "mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_index = DenseFlatIndexer(1024, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f564f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_batch_1 = mean.iloc[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(mean_batch_1.values).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d25e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_batch_1.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_index.index_data(np.stack(mean_batch_1.values).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_batch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_df.query('wikipedia_id == 341466')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mention = encodings_df.loc[14882]['encoding'].astype('float32')\n",
    "new_mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack([new_mention])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63541702",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_index.search_knn(np.stack([new_mention]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(mean_batch_1.values[2], new_mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_batch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_batch_2 = mean.iloc[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe534f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(mean_batch_2.values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53218cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_batch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_df.query('wikipedia_id == 5945')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b3711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92228551",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mention2 = np.stack(encodings_df.loc[452].query('wikipedia_id == 5945')['encoding'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_index.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88fe971",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_index.index_data(np.stack(mean_batch_2).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83259c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_index.search_knn(new_mention2.astype('float32'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40958add",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_batch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10aeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corretto. todo setup ambiente di testing"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.11.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
