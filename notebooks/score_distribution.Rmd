---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.4
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
import pandas as pd
import json
import os
from glob import glob
import statistics
from matplotlib import pyplot
from math import isnan
```

```{python}
# !ls -ld ../output* 
```

```{python}
output_path = os.path.join('..', 'output_20200805')
```

```{python}
bi_scores = glob(os.path.join(output_path, '*_bi.jsonl'))
bi_scores
```

```{python}
cross_scores = glob(os.path.join(output_path, '*_cross.jsonl'))
cross_scores
```

```{python}
bi_df = pd.DataFrame()
for score_file in bi_scores:
    bi_df = pd.concat([bi_df, pd.read_json(score_file)])
    
assert (bi_df['labels'].apply(lambda x: len(x)) != 1).sum() == 0
bi_df['labels'] = bi_df['labels'].apply(lambda x: x[0])
bi_df
```

```{python}
def _bi_get_stats(x):
    assert len(x.scores) == len(x.nns)
    uncorrect = x.scores.copy()
    if x.labels in x.nns:
        # found correct entity
        i_correct = x.nns.index(x.labels)
        correct = x.scores[i_correct]
        del uncorrect[i_correct]
    else:
        # not found correct entity
        correct = None
    _stats = {
        "correct": correct,
        "max": max(uncorrect),
        "min": min(uncorrect),
        "mean": statistics.mean(uncorrect),
        "median": statistics.median(uncorrect),
        "stdev": statistics.stdev(uncorrect),
    }
    return _stats
```

```{python}
bi_stats = bi_df.apply(_bi_get_stats, axis=1, result_type='expand')
bi_stats.head()
```

```{python}
(bi_stats['max'] - bi_stats['min']).plot(kind='density')
```

```{python}
bi_stats['stdev'].plot(kind='density')
```

```{python}
bi_stats['stdev'].describe()
```

```{python}
# correct is not na
bi_stats[bi_stats['correct'].notna()][['correct', 'max', 'mean', 'median', 'min']].plot(kind='density')
```

```{python}
# correct not found
bi_stats[bi_stats['correct'].isna()][['max', 'mean', 'median', 'min']].plot(kind='density')
```

```{python}
cross_df = pd.DataFrame()
for score_file in cross_scores:
    cross_df = pd.concat([cross_df, pd.read_json(score_file)])
    
assert (cross_df['labels'].apply(lambda x: len(x)) != 1).sum() == 0
cross_df['labels'] = cross_df['labels'].apply(lambda x: x[0])
cross_df
```

```{python}
x = cross_df.iloc[0]
```

```{python}
def _cross_get_stats(x):
    assert len(x.unsorted_scores) == len(x.nns)
    uncorrect = x.unsorted_scores.copy()
    if x.labels in x.nns:
        # found correct entity
        i_correct = x.nns.index(x.labels)
        correct = x.unsorted_scores[i_correct]
        del uncorrect[i_correct]
    else:
        # not found correct entity
        correct = None
    _stats = {
        "correct": correct,
        "max": max(uncorrect),
        "min": min(uncorrect),
        "mean": statistics.mean(uncorrect),
        "median": statistics.median(uncorrect),
        "stdev": statistics.stdev(uncorrect),
    }
    return _stats
```

```{python}
cross_stats = cross_df.apply(_cross_get_stats, axis=1, result_type='expand')
cross_stats.head()
```

```{python}
# only the mentions we found the correct entity
# sort and remove na
cross_stats_s = cross_stats[cross_stats['correct'].notna()].sort_values('correct')
# plot
pyplot.scatter(x=range(cross_stats_s.shape[0]), y=cross_stats_s['correct'], s=0.01, c='blue')
pyplot.scatter(x=range(cross_stats_s.shape[0]), y=cross_stats_s['max'], s=0.01, c='green')
pyplot.scatter(x=range(cross_stats_s.shape[0]), y=cross_stats_s['mean'], s=0.01, c='orange')
#pyplot.scatter(x=range(cross_stats_s.shape[0]), y=cross_stats_s['median'], s=0.01, c='red')
pyplot.legend(
    labels = ['correct', 'max', 'mean'],
    labelcolor = ['blue', 'green', 'orange']
) 
```

```{python}
# only the mentions we found the correct entity # unsorted
# sort and remove na
cross_stats_u = cross_stats[cross_stats['correct'].notna()]
# plot
pyplot.scatter(x=range(cross_stats_u.shape[0]), y=cross_stats_u['correct'], s=0.01, c='blue')
pyplot.scatter(x=range(cross_stats_u.shape[0]), y=cross_stats_u['max'], s=0.01, c='red')
pyplot.scatter(x=range(cross_stats_u.shape[0]), y=cross_stats_u['mean'], s=0.01, c='yellow')
#pyplot.scatter(x=range(cross_stats_u.shape[0]), y=cross_stats_u['median'], s=0.01, c='orange')
pyplot.legend(
    labels = ['correct', 'max', 'mean'],
    labelcolor = ['blue', 'red', 'yellow']
) 
```

The first mentions on the left (they are sorted basing on correct score) could be misclassified as NIL by a threshold based classifier.

```{python}
# the mentions we didn't find the correct entity
# sort and remove na
cross_stats_na = cross_stats[cross_stats['correct'].isna()]
# plot
#pyplot.scatter(x=range(cross_stats_s_na.shape[0]), y=cross_stats_na['correct'], s=1, c='blue')
pyplot.scatter(x=range(cross_stats_na.shape[0]), y=cross_stats_na['max'], s=1, c='green')
pyplot.scatter(x=range(cross_stats_na.shape[0]), y=cross_stats_na['mean'], s=1, c='orange')
pyplot.scatter(x=range(cross_stats_na.shape[0]), y=cross_stats_na['median'], s=1, c='red')
pyplot.scatter(x=range(cross_stats_na.shape[0]), y=cross_stats_na['min'], s=1, c='blue')
pyplot.legend(
    labels = ['max', 'mean', 'median', 'min'],
    labelcolor = ['green', 'orange', 'red', 'blue']
)
```

Average score of uncorrect entities is around -10

```{python}
# correct found
cross_stats[cross_stats['correct'].notna()][['correct', 'max', 'mean', 'median']].plot(kind='density')
```

```{python}
# correct not found
cross_stats[cross_stats['correct'].isna()][['max', 'mean', 'median']].plot(kind='density')
```

```{python}
cross_stats['stdev'].describe()
```

```{python}
(cross_stats['max'] - cross_stats['min']).plot(kind='density')
```

```{python}
cross_stats['min'].describe()
```

# NIL

```{python}
output_path_nil = os.path.join('..', 'output_only_NIL_20200806')
```

```{python}
bi_scores_nil = glob(os.path.join(output_path_nil, '*_bi.jsonl'))
bi_scores_nil
```

```{python}
cross_scores_nil = glob(os.path.join(output_path_nil, '*_cross.jsonl'))
cross_scores_nil
```

```{python}
bi_df_nil = pd.DataFrame()
for score_file in bi_scores_nil:
    bi_df_nil = pd.concat([bi_df_nil, pd.read_json(score_file)])
    
assert (bi_df_nil['labels'].apply(lambda x: len(x)) != 1).sum() == 0
bi_df_nil['labels'] = bi_df_nil['labels'].apply(lambda x: x[0])
bi_df_nil
```

```{python}
def _bi_get_stats(x):
    assert len(x.scores) == len(x.nns)
    uncorrect = x.scores.copy()
    if x.labels in x.nns:
        # found correct entity
        i_correct = x.nns.index(x.labels)
        correct = x.scores[i_correct]
        del uncorrect[i_correct]
    else:
        # not found correct entity
        correct = None
    _stats = {
        "correct": correct,
        "max": max(uncorrect),
        "min": min(uncorrect),
        "mean": statistics.mean(uncorrect),
        "median": statistics.median(uncorrect),
        "stdev": statistics.stdev(uncorrect),
    }
    return _stats
```

```{python}
bi_stats_nil = bi_df_nil.apply(_bi_get_stats, axis=1, result_type='expand')
bi_stats_nil.head()
```

```{python}
(bi_stats_nil['max'] - bi_stats_nil['min']).plot(kind='density')
```

```{python}
bi_stats_nil['stdev'].plot(kind='density')
```

```{python}
bi_stats_nil['stdev'].describe()
```

```{python}
assert bi_stats_nil['correct'].notna().sum() == 0
```

```{python}
# correct not found # all of NIL
bi_stats_nil[['max', 'mean', 'median', 'min']].plot(kind='density')
```

```{python}
cross_df_nil = pd.DataFrame()
for score_file in cross_scores_nil:
    cross_df_nil = pd.concat([cross_df_nil, pd.read_json(score_file)])
    
assert (cross_df_nil['labels'].apply(lambda x: len(x)) != 1).sum() == 0
cross_df_nil['labels'] = cross_df_nil['labels'].apply(lambda x: x[0])
cross_df_nil
```

```{python}
def _cross_get_stats(x):
    assert len(x.unsorted_scores) == len(x.nns)
    uncorrect = x.unsorted_scores.copy()
    if x.labels in x.nns:
        # found correct entity
        i_correct = x.nns.index(x.labels)
        correct = x.unsorted_scores[i_correct]
        del uncorrect[i_correct]
    else:
        # not found correct entity
        correct = None
    _stats = {
        "correct": correct,
        "max": max(uncorrect),
        "min": min(uncorrect),
        "mean": statistics.mean(uncorrect),
        "median": statistics.median(uncorrect),
        "stdev": statistics.stdev(uncorrect),
    }
    return _stats
```

```{python}
cross_stats_nil = cross_df_nil.apply(_cross_get_stats, axis=1, result_type='expand')
cross_stats_nil.head()
```

```{python}
# the mentions we didn't find the correct entity # all
# plot
pyplot.scatter(x=range(cross_stats_nil.shape[0]), y=cross_stats_nil['max'], s=0.1, c='green')
pyplot.scatter(x=range(cross_stats_nil.shape[0]), y=cross_stats_nil['mean'], s=0.1, c='orange')
pyplot.scatter(x=range(cross_stats_nil.shape[0]), y=cross_stats_nil['median'], s=0.1, c='red')
pyplot.scatter(x=range(cross_stats_nil.shape[0]), y=cross_stats_nil['min'], s=0.1, c='blue')
pyplot.legend(
    labels = ['max', 'mean', 'median', 'min'],
    labelcolor = ['green', 'orange', 'red', 'blue']
)
```

```{python}
# correct not found # all nil
cross_stats_nil[['max', 'mean', 'median']].plot(kind='density')
```

```{python}
cross_stats_nil['stdev'].describe()
```

```{python}
(cross_stats_nil['max'] - cross_stats_nil['min']).plot(kind='density')
```

```{python}
cross_stats_nil['min'].describe()
```

```{python}
cross_stats_nil['max'].describe()
```

```{python}
cross_stats_nil['max'].plot(kind='density')
```

```{python}
cross_stats['correct'].plot(kind='density')
```

threshold should accept most cross_stats['correct'] and reject most cross_stats_nil['max']

```{python}
# create dataset


```

```{python}
cross_df
```

```{python}
positives = cross_stats.apply(lambda x: x['correct'] if not isnan(x['correct']) and x['correct'] > x['max'] else None, axis=1).dropna()
```

```{python}
negatives2 = cross_stats.apply(lambda x: x['max'] if isnan(x['correct']) else None, axis=1).dropna()
```

```{python}
negatives = cross_stats_nil['max']
```

```{python}
th_df = pd.DataFrame({"x": positives, "y": [1] * len(positives)})
```

```{python}
th_df = pd.concat([th_df, pd.DataFrame({"x": negatives, "y": [0] * len(negatives)})])
th_df = pd.concat([th_df, pd.DataFrame({"x": negatives2, "y": [0] * len(negatives2)})])
```

```{python}
th_df['y'].value_counts()
```

```{python}
th_df[th_df['y'] == 1]['x'].plot(kind='density')
```

```{python}
th_df[th_df['y'] == 0]['x'].plot(kind='density')
```

```{python}
from sklearn.linear_model import LinearRegression
import numpy as np
```

```{python}
reg = LinearRegression().fit(th_df['x'].array.reshape(-1, 1), th_df['y'])
```

```{python}
reg.score(th_df['x'].array.reshape(-1, 1), th_df['y'])
```

```{python}
reg.predict([[-15]])
```

```{python}
reg.coef_
```

```{python}
th_df['y_pred_lr'] = list(map(lambda x: 1 if x >= 0.5 else 0, reg.predict(th_df['x'].array.reshape(-1, 1))))
```

Evaluation (true is not-NIL)

```{python}
def _eval(df, y_pred, y='y', title=None):
    ACC = (df['y'] == df[y_pred]).sum() / df.shape[0]
    TPR = df.query(f'y==1 and {y_pred} ==1').count()[0] / df.query('y==1').shape[0]
    TNR = df.query(f'y==0 and {y_pred} ==0').count()[0] / df.query(f'y==0').shape[0]
    FNR = 1 - TPR
    PPV = df.query(f'y==1 and {y_pred} ==1').count()[0] / df.query(f'{y_pred}==1').shape[0]
    NPV = df.query(f'y==0 and {y_pred} ==0').count()[0] / df.query(f'{y_pred}==0').shape[0]
    FPR = 1 - TNR
    FDR = 1 - PPV
    FOR = 1 - NPV
    F1 = 2*(PPV*TPR)/(PPV+TPR)
    return pd.DataFrame({
        "ACC": [ACC],
        "TPR": [TPR],
        "TNR": [TNR],
        "FNR": [FNR],
        "PPV": [PPV],
        "NPV": [NPV],
        "FPR": [FPR],
        "FDR": [FDR],
        "FOR": [FOR],
        "F1": [F1],        
    }, index = [y_pred if title is None else title])
```

```{python}
eval_df = _eval(th_df, 'y_pred_lr')
eval_df
```

several mentions are classified as not-NIL even if they should be NIL;
- low TNR
- high FPR
- high FOR

think about class imbalance


### Class imbalance
we have few NILs and a lot more correct examples.

but for each correct example we have the best but wrong entities;
we can create as many NIL example as we need.

For NIL example I mean a pair mention-entity which is not correct
and therefore should be classified as NIL or not-correct.

```{python}
n_augment = (lambda x: abs(x[0]-x[1]))(th_df['y'].value_counts())
n_augment
```

```{python}
# take n_augment random samples from positives
from sklearn.utils.random import sample_without_replacement
```

```{python}
_pos_to_sample = cross_stats.apply(lambda x: x if not isnan(x['correct']) and x['correct'] > x['max'] else None, axis=1).dropna()
```

```{python}
_sample = _pos_to_sample.iloc[sample_without_replacement(_pos_to_sample.shape[0], n_augment)]
_sample.head()
```

```{python}
negatives_a = _sample['max']
```

```{python}
th_df_a = pd.concat([th_df, pd.DataFrame({"x": negatives_a, "y": [0] * len(negatives_a)})])
th_df_a
```

```{python}
th_df_a['y'].value_counts()
```

```{python}
reg_a = LinearRegression().fit(th_df_a['x'].array.reshape(-1, 1), th_df_a['y'])
```

```{python}
th_df['y_pred_lr_a'] = list(map(lambda x: 1 if x >= 0.5 else 0, reg_a.predict(th_df['x'].array.reshape(-1, 1))))
```

```{python}
eval_df = pd.concat([eval_df, _eval(th_df, 'y_pred_lr_a')])
eval_df
```

### evaluation metrics
specially TPR TNR PPV NPV

with balanced class TNR reached 0.68
NPV is low (we have FN)


# TODO
- [ok but with LinearRegression] try to fix a threshold for the NIL classification to maximize performance
- [ok] LinearRegression as baseline + LinearRegression trained on augmented/balanced dataset
- [ok for now] how to evaluate various methods/metrics
- [ok] think about class imbalance
- consider all the top_k results instead of only the top_1
