---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.4
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import pickle
from sklearn.metrics import confusion_matrix, classification_report
import sklearn

import matplotlib.pyplot as plt
```

```{python}
whole2 = pd.read_csv('whole2_with_100_10_stats_ds.csv', index_col=0)
whole2.head()
```

```{python}
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# train data


class trainData(Dataset):

    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data

    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]

    def __len__(self):
        return len(self.X_data)


class testData(Dataset):

    def __init__(self, X_data):
        self.X_data = X_data

    def __getitem__(self, index):
        return self.X_data[index]

    def __len__(self):
        return len(self.X_data)


class binaryClassification(nn.Module):
    def __init__(self, n):
        super(binaryClassification, self).__init__()
        self.fc1 = nn.Linear(n, 2)
        self.fc2 = nn.Linear(2, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.1)

    def forward(self, inputs):
        x = self.fc1(inputs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        #x = nn.Sigmoid(x)

        return x

def binary_acc(y_pred, y_test):
    y_pred_tag = torch.round(torch.sigmoid(y_pred))

    correct_results_sum = (y_pred_tag == y_test).sum().float()
    acc = correct_results_sum/y_test.shape[0]
    acc = torch.round(acc * 100)

    return acc
```

```{python}
def _nrl(datasets, title='nrl', features=['max_cross'], EPOCHS=100, BATCH_SIZE=64, LEARNING_RATE=0.0001):
    print('nrl-{} ...'.format('+'.join(features)))
    for k in ['train', 'train_hard', 'train_aug', 'train_hard_aug']:
        print('train on', k)
        d = datasets[k]
        assert d[features].isna().sum().sum() == 0
        scaler = StandardScaler()

        X_train = d[features].values
        y_train = d['y'].values

        X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42)

        X_train = scaler.fit_transform(X_train)
        X_validation = scaler.transform(X_validation)

        X_validation = torch.FloatTensor(X_validation)
        y_validation = torch.FloatTensor(y_validation.reshape(-1, 1))

        model = binaryClassification(len(features))
        model.to(device)

        criterion = nn.BCEWithLogitsLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

        train_data = trainData(torch.FloatTensor(X_train),
                        torch.FloatTensor(y_train))
        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)

        model.train()
        for e in range(1, EPOCHS+1):
            epoch_loss = 0
            epoch_acc = 0
            for X_batch, y_batch in train_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                optimizer.zero_grad()

                y_pred = model(X_batch)

                loss = criterion(y_pred, y_batch.unsqueeze(1))

                with torch.no_grad():
                    y_pred_validation = model(X_validation)
                    val_loss = float(F.binary_cross_entropy_with_logits(y_pred_validation, y_validation))

                acc = binary_acc(y_pred, y_batch.unsqueeze(1))

                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()
                epoch_acc += acc.item()

            if e % 5 == 0:
                print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Val Loss: {val_loss:.5f} | Acc: {epoch_acc/len(train_loader):.3f}')

        if _save_models:
            with open(os.path.join(_save_models_path, 'stdscaler+{}+{}.pkl'.format('+'.join(features), k)), 'wb') as fd:
                pickle.dump(scaler, fd)
            torch.save(model.state_dict(), os.path.join(_save_models_path, '{}+{}+{}.torch'.format(title, '+'.join(features), k)))

        for tk in ['test', 'test_hard']:
            print('test on', tk)
            t = datasets[tk]

            X_test = t[features].values
            X_test = scaler.transform(X_test)

            y_test = t['y']

            test_data = testData(torch.FloatTensor(X_test))
            test_loader = DataLoader(dataset=test_data, batch_size=1)

            y_pred_list = []
            model.eval()
            with torch.no_grad():
                for X_batch in test_loader:
                    X_batch = X_batch.to(device)
                    y_test_pred = model(X_batch)
                    y_test_pred = torch.sigmoid(y_test_pred)
                    y_pred_tag = torch.round(y_test_pred)
                    y_pred_list.append(y_pred_tag.cpu().numpy())
            y_pred_list = [a.squeeze().tolist() for a in y_pred_list]

            _temp = pd.DataFrame()
            _temp['y'] = y_test
            _temp['y_pred'] = y_pred_list

            yield _eval(_temp, 'y_pred', title='{}+{}+{}+{}'.format(title, '+'.join(features), k, tk))

```

```{python}
list(whole2.columns)
```

```{python}
nil100_columns = [
 'cross_stats_100_max',
 'cross_stats_100_second',
 'cross_stats_100_min',
 'cross_stats_100_mean',
 'cross_stats_100_median',
 'cross_stats_100_stdev',
 'bi_stats_100_max',
 'bi_stats_100_second',
 'bi_stats_100_min',
 'bi_stats_100_mean',
 'bi_stats_100_median',
 'bi_stats_100_stdev',
 'cross_jaccard',
 'cross_damerau_levenshtein',
 'bi_jaccard',
 'bi_damerau_levenshtein'
]
```

```{python}
train_df = whole2
```

```{python}
def eval_plot(df):
    print(classification_report(df['y'], df['y_pred_rnd']))
    print('y kde')
    df[['y', 'y_pred']].plot(kind='kde')
    # roc curve
    fpr, tpr, thresholds = sklearn.metrics.roc_curve(df['y'], df['y_pred'])
    pd.DataFrame({
        'tpr': tpr,
        'fpr': fpr,
        'thresholds': thresholds
    }).plot(x='fpr', y='tpr', title='roc curve')
    plt.show()
    print('--- correct ---')
    # correct
    correct = df.query('y == y_pred_rnd')['y_pred']
    correct.plot(kind='density', title='correct kde')
    plt.show()
    correct.plot(kind='hist', title='correct hist')
    plt.show()
    print('correct description:')
    print(correct.describe())
    print('-- errors ---')
    # errors
    errors = df.query('y != y_pred_rnd')['y_pred']
    errors.plot(kind='density', title='errors kde')
    plt.show()
    errors.plot(kind='hist', title='errors hist')
    plt.show()
    print('errors description:')
    print(errors.describe())
```

```{python}
# quando deve essere nil e quando cross sbaglia
train_df['y'] = ((train_df['cross_labels'] != -1) & (train_df['cross_labels'] == train_df['cross_best_candidate'])).astype(int)
```

```{python}
train_df['y'].value_counts()
```

```{python}
train_df, test_df = train_test_split(train_df, test_size=0.33, random_state=17)
```

```{python}
downsample = True

if downsample:
    train_df_0 = train_df.query('y == 0')
    train_df_1 = train_df.query('y == 1')

    train_df_1 = train_df_1.sample(frac=1).iloc[:train_df_0.shape[0]]
    train_df = pd.concat([train_df_0, train_df_1]).sample(frac=1)
```

```{python}
train_df['y'].value_counts()
```

```{python}
features = nil100_columns
```

```{python}
X_train = train_df[features].values
y_train = train_df['y'].values
```

```{python}
X_test = test_df[features].values
y_test = test_df['y'].values
```

```{python}
X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=17)
```

```{python}
print(X_train.shape)
print(X_validation.shape)
print(X_test.shape)
```

```{python}
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_validation = scaler.transform(X_validation)

X_validation = torch.FloatTensor(X_validation)
y_validation = torch.FloatTensor(y_validation.reshape(-1, 1))
```

```{python}
X_test = scaler.transform(X_test)
```

```{python}
BATCH_SIZE = 64
LEARNING_RATE = 0.00005
EPOCHS = 200
```

```{python}
loss_df = pd.DataFrame(columns=['loss', 'val_loss', 'acc'])

model = binaryClassification(len(features))
model.to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

train_data = trainData(torch.FloatTensor(X_train),
                torch.FloatTensor(y_train))
train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)

model.train()
for e in range(1, EPOCHS+1):
    epoch_loss = 0
    epoch_acc = 0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()

        y_pred = model(X_batch)

        loss = criterion(y_pred, y_batch.unsqueeze(1))

        with torch.no_grad():
            y_pred_validation = model(X_validation)
            val_loss = float(F.binary_cross_entropy_with_logits(y_pred_validation, y_validation))

        acc = binary_acc(y_pred, y_batch.unsqueeze(1))

        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc.item()
        
        loss_df.loc[e] = {
            'loss': epoch_loss/len(train_loader),
            'val_loss': val_loss,
            'acc': epoch_acc/len(train_loader)
        }

    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Val Loss: {val_loss:.5f} | Acc: {epoch_acc/len(train_loader):.3f}')

```

```{python}
loss_df.plot(y=['loss', 'val_loss'])
```

```{python}
model.eval()
y_pred_list = torch.sigmoid(model(torch.FloatTensor(X_test))).cpu().detach().numpy().reshape(-1,)
```

```{python}
eval_df = pd.DataFrame({
    'y': y_test,
    'y_pred': y_pred_list,
    'y_pred_rnd': np.round(y_pred_list)
})
eval_df.head()
```

```{python}
eval_plot(eval_df)
```

```{python}

```
