---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.4
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
from prepare_dataset import _load_datasets
import pandas as pd
```

```{python}
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
```

```{python}
datasets = _load_datasets('dataset')
datasets
```

```{python}
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
```

```{python}
## train data
class trainData(Dataset):
    
    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data
        
    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]
        
    def __len__ (self):
        return len(self.X_data)
```

```{python}
class testData(Dataset):
    
    def __init__(self, X_data):
        self.X_data = X_data
        
    def __getitem__(self, index):
        return self.X_data[index]
        
    def __len__ (self):
        return len(self.X_data)
```

```{python}
class binaryClassification(nn.Module):
    def __init__(self, n):
        super(binaryClassification, self).__init__()
        self.fc1 = nn.Linear(n, 2)
        self.fc2 = nn.Linear(2, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.1)
        
    def forward(self, inputs):
        x = self.fc1(inputs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        #x = nn.Sigmoid(x)
        
        return x
```

```{python}
def binary_acc(y_pred, y_test):
    y_pred_tag = torch.round(torch.sigmoid(y_pred))

    correct_results_sum = (y_pred_tag == y_test).sum().float()
    acc = correct_results_sum/y_test.shape[0]
    acc = torch.round(acc * 100)
    
    return acc
```

```{python}
features = ['max_cross']
```

```{python}
EPOCHS = 50
BATCH_SIZE = 64
LEARNING_RATE = 0.0001
```

```{python}
models = {}
_save_models = True

for k in ['train', 'train_hard', 'train_aug', 'train_hard_aug']:
    print('train on', k)
    d = datasets[k]
    assert d[features].isna().sum().sum() == 0
    scaler = StandardScaler()
    
    X_train = d[features].values
    X_train = scaler.fit_transform(X_train)
    
    y_train = d['y'].values
    
    model = binaryClassification(len(features))
    model.to(device)
    
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    train_data = trainData(torch.FloatTensor(X_train), 
                       torch.FloatTensor(y_train))
    train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)
    
    model.train()
    for e in range(1, EPOCHS+1):
        epoch_loss = 0
        epoch_acc = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()

            y_pred = model(X_batch)

            loss = criterion(y_pred, y_batch.unsqueeze(1))
            acc = binary_acc(y_pred, y_batch.unsqueeze(1))

            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            epoch_acc += acc.item()

        if e % 5 == 0:
            print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')

    if _save_models:
        models[k] = model

    for tk in ['test', 'test_hard']:
        print('test on', tk)
        t = datasets[tk]
        
        X_test = t[features].values
        X_test = scaler.transform(X_test)
        
        y_test = t['y']
        
        test_data = testData(torch.FloatTensor(X_test))
        test_loader = DataLoader(dataset=test_data, batch_size=1)
        
        y_pred_list = []
        model.eval()
        with torch.no_grad():
            for X_batch in test_loader:
                X_batch = X_batch.to(device)
                y_test_pred = model(X_batch)
                y_test_pred = torch.sigmoid(y_test_pred)
                y_pred_tag = torch.round(y_test_pred)
                y_pred_list.append(y_pred_tag.cpu().numpy())
        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]
        
        print(classification_report(y_test, y_pred_list))
```

```{python}
t = datasets['test_hard_aug']
        
X_test = t[features].values
X_test = scaler.transform(X_test)

y_test = t['y']
y_pred = torch.sigmoid(models['train_hard_aug'](torch.FloatTensor(X_test)))
```

```{python}
assert len(y_test) == len(y_pred)
eval_df = pd.DataFrame({
    'y':y_test,
    'y_pred': y_pred.detach().numpy().reshape(-1,),
    'y_pred_rnd': torch.round(y_pred).detach().numpy().reshape(-1,).astype(int)
})
eval_df.head()
```

```{python}
# correct
correct = eval_df.query('y == y_pred_rnd')['y_pred']
correct.plot(kind='density')
correct.describe()
```

```{python}
correct.plot(kind='hist')
```

```{python}
# errors
errors = eval_df.query('y != y_pred_rnd')['y_pred']
errors.plot(kind='density')
errors.describe()
```

```{python}
errors.plot(kind='hist')
```
