---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.4
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd
import statistics
import pickle
import os
```

```{python}
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report

import sklearn.metrics
import os
import matplotlib.pyplot as plt
import pickle

from sklearn.model_selection import train_test_split
```

```{python}
# ls ../output* -d

```

```{python}
basepath = '../output_ip_20200905/'
```

```{python}
datasets = [
    'AIDA-YAGO2_testa_scores',
    'AIDA-YAGO2_testb_scores',
    'AIDA-YAGO2_train_scores',
    'ace2004_questions_scores',
    'aquaint_questions_scores',
    'clueweb_questions_scores',
    'msnbc_questions_scores',
    'wnedwiki_questions_scores'
]
```

```{python}
def _bi_get_stats(x, remove_correct = False):
    assert len(x.scores) == len(x.nns)
    scores = x.scores.copy()
    correct = None
    if x.labels in x.nns:
        # found correct entity
        i_correct = x.nns.index(x.labels)
        correct = scores[i_correct]

    _stats = {
        "correct": correct,
        "max": max(scores),
        "second": sorted(scores, reverse=True)[1],
        "min": min(scores),
        "mean": statistics.mean(scores),
        "median": statistics.median(scores),
        "stdev": statistics.stdev(scores)
    }
    return _stats

def _cross_get_stats(x, remove_correct=False):
    assert len(x.unsorted_scores) == len(x.nns)
    scores = x.unsorted_scores.copy()
    correct = None
    if x.labels in x.nns:
        # found correct entity
        i_correct = x.nns.index(x.labels)
        correct = scores[i_correct]

    _stats = {
        "correct": correct,
        "max": max(scores),
        "second": sorted(scores, reverse=True)[1],
        "min": min(scores),
        "mean": statistics.mean(scores),
        "median": statistics.median(scores),
        "stdev": statistics.stdev(scores),
    }
    return _stats

```

```{python}
def _load_scores(bi_scores, cross_scores, basepath='.'):
    bi_scores = os.path.join(basepath, bi_scores)
    cross_scores = os.path.join(basepath, cross_scores)

    bi_df = pd.read_json(bi_scores)

    assert (bi_df['labels'].apply(lambda x: len(x)) != 1).sum() == 0
    bi_df['labels'] = bi_df['labels'].apply(lambda x: x[0])

    bi_stats = bi_df.apply(_bi_get_stats, axis=1, result_type='expand')

    cross_df = pd.read_json(cross_scores)

    assert (cross_df['labels'].apply(lambda x: len(x)) != 1).sum() == 0
    cross_df['labels'] = cross_df['labels'].apply(lambda x: x[0])

    cross_stats = cross_df.apply(_cross_get_stats, axis=1, result_type='expand')

    assert all(bi_df['labels'] == cross_df['labels'])

    combined_stats = bi_stats.copy()
    combined_stats.columns = [c+'_bi' for c in combined_stats.columns]
    combined_stats[[c+'_cross' for c in cross_stats.columns]] = cross_stats

    combined_stats['idx'] = combined_stats.index

    return bi_stats, cross_stats, combined_stats, bi_df, cross_df
```

```{python}
import numpy as np
def _eval_line(x, scores_col='scores'):
    assert len(x[scores_col]) == len(x.nns)
    scores = x[scores_col].copy()
    correct = -1
    if x.labels in x.nns:
        # found correct entity
        i_correct = x.nns.index(x.labels)
        # correct is position of the correct entity according to the estimated score
        # correct = 0 means the best scored entity is the correct one
        # correct = -1 means the correct entity is not in the top k
        correct = np.argsort(x[scores_col])[::-1].tolist().index(i_correct)

    return correct
```

```{python}
def _best_candidate(scores, nns, nil_score=None, nil_threshold=0.5):
    if nil_score is not None and nil_score < nil_threshold:
        # identified as NIL
        return -1
    else:
        return nns[np.argmax(scores)]        
```

```{python}
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
```

```{python}
## train data
class trainData(Dataset):
    
    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data
        
    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]
        
    def __len__ (self):
        return len(self.X_data)
```

```{python}
class testData(Dataset):
    
    def __init__(self, X_data):
        self.X_data = X_data
        
    def __getitem__(self, index):
        return self.X_data[index]
        
    def __len__ (self):
        return len(self.X_data)
```

```{python}
class binaryClassification(nn.Module):
    def __init__(self, n):
        super(binaryClassification, self).__init__()
        self.fc1 = nn.Linear(n, 2)
        self.fc2 = nn.Linear(2, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.1)
        
    def forward(self, inputs):
        x = self.fc1(inputs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        #x = nn.Sigmoid(x)
        
        return x
```

```{python}
nil_scaler_bi_path = 'models_nb_20210921_bi/stdscaler+max_bi+second_bi+min_bi+mean_bi+median_bi+stdev_bi+train_hard_aug.pkl'
nil_model_bi_path = 'models_nb_20210921_bi/nrl+max_bi+second_bi+min_bi+mean_bi+median_bi+stdev_bi+train_hard_aug.torch'
features_bi = [
    'max', 'second', 'min', 'mean', 'median', 'stdev'
]

nil_scaler_path = 'models_nb_20210921/stdscaler+max_bi+second_bi+min_bi+mean_bi+median_bi+stdev_bi+max_cross+second_cross+min_cross+mean_cross+median_cross+stdev_cross+train_hard_aug.pkl'
nil_model_path = 'models_nb_20210921/nrl+max_bi+second_bi+min_bi+mean_bi+median_bi+stdev_bi+max_cross+second_cross+min_cross+mean_cross+median_cross+stdev_cross+train_hard_aug.torch'
features = [
    'max_bi', 'second_bi', 'min_bi', 'mean_bi', 'median_bi',
    'stdev_bi', 'max_cross', 'second_cross', 'min_cross',
    'mean_cross', 'median_cross', 'stdev_cross'
]

with open(nil_scaler_bi_path, 'rb') as fd:
    nilclf_scaler_bi = pickle.load(fd)
    
with open(nil_scaler_path, 'rb') as fd:
    nilclf_scaler = pickle.load(fd)

nilclf_model_bi = binaryClassification(len(features_bi))
nilclf_model_bi.load_state_dict(torch.load(nil_model_bi_path))

nilclf_model = binaryClassification(len(features))
nilclf_model.load_state_dict(torch.load(nil_model_path))

def nilclf_bi(x):
    x = nilclf_scaler_bi.transform(x)
    x = torch.FloatTensor(x)
    y = nilclf_model_bi(x)
    y = torch.sigmoid(y)
    y = y.detach().numpy().reshape(-1,)
    return y

def nilclf(x):
    x = nilclf_scaler.transform(x)
    x = torch.FloatTensor(x)
    y = nilclf_model(x)
    y = torch.sigmoid(y)
    y = y.detach().numpy().reshape(-1,)
    return y
```

```{python}
report = pd.DataFrame(columns = [
    'bi_acc',
    'bi_acc_nil',
    'bi_recall10',
    'bi_recall30',
    'bi_recall100',
    'cross_norm_acc',
    'overall_unn_acc',
    'cross_norm_acc_nil',
    'overall_unn_acc_nil',
    'nil_p_bi',
    'nil_r_bi',
    'nil_f1_bi',
    'nil_acc_bi',
    'nil_p_cross',
    'nil_r_cross',
    'nil_f1_cross',
    'nil_acc_cross'
])
for d in datasets:
    bi_path = f'{d}_bi.jsonl'
    cross_path = f'{d}_cross.jsonl'
    bi_stats, _, combined_stats, bi_df, cross_df = _load_scores(bi_path, cross_path, basepath)
    assert bi_df.shape[0] == cross_df.shape[0]

    bi_df['recall@'] = bi_df.apply(lambda x: _eval_line(x, 'scores'), axis=1)
    bi_df['best_candidate'] = bi_df.apply(lambda x: _best_candidate(x['scores'], x.nns), axis=1)
    
    bi_df['nil_p'] = nilclf_bi(bi_stats[features_bi].values)
    bi_df['nil_b'] = np.round(bi_df['nil_p'])

    bi_df['best_candidate+nil'] = bi_df.apply(
        lambda x: _best_candidate(x['scores'], x.nns, x['nil_b']), axis=1)

    biencoder_accuracy = bi_df.query('`recall@` == 0').shape[0] / bi_df.shape[0]
    biencoder_accuracy_nil = bi_df.query('labels == `best_candidate+nil`').shape[0] / bi_df.shape[0]

    biencoder_recall10 = bi_df.query('`recall@` >= 0 and `recall@` < 10').shape[0] / bi_df.shape[0]
    biencoder_recall30 = bi_df.query('`recall@` >= 0 and `recall@` < 30').shape[0] / bi_df.shape[0]
    biencoder_recall100 = bi_df.query('`recall@` >= 0 and `recall@` < 100').shape[0] / bi_df.shape[0]
    
    nil_y_bi = (bi_df['labels'] != -1).astype(int).values
    nil_y_bi_pred = bi_df['nil_b'].values
    
    nil_report_bi = classification_report(nil_y_bi, nil_y_bi_pred, output_dict=True)

    cross_df['recall@'] = cross_df.apply(lambda x: _eval_line(x, 'unsorted_scores'), axis=1)
    cross_df['best_candidate'] = cross_df.apply(lambda x: _best_candidate(x['unsorted_scores'], x.nns), axis=1)
    
    cross_df['nil_p'] = nilclf(combined_stats[features].values)
    cross_df['nil_b'] = np.round(cross_df['nil_p'])

    cross_df['best_candidate+nil'] = cross_df.apply(
        lambda x: _best_candidate(x['unsorted_scores'], x.nns, x['nil_b']), axis=1)
    
    overall_unnormalized_accuracy = cross_df.query('`recall@` == 0').shape[0] / cross_df.shape[0]
    overall_unnormalized_accuracy_nil = cross_df.query('labels == `best_candidate+nil`').shape[0] / cross_df.shape[0]
    
    cross_df_fnd = cross_df[cross_df.apply(lambda x: x.labels in x.nns, axis=1)]
    
    crossencoder_normalized_accuracy = cross_df_fnd.query('`recall@` == 0').shape[0] / cross_df_fnd.shape[0]
    crossencoder_normalized_accuracy_nil = cross_df_fnd.query('labels == `best_candidate+nil`').shape[0] / cross_df_fnd.shape[0]

    nil_y_cross = (cross_df['labels'] != -1).astype(int).values
    nil_y_cross_pred = cross_df['nil_b'].values
    
    nil_report_cross = classification_report(nil_y_cross, nil_y_cross_pred, output_dict=True)

    bi_not_nil = bi_df.query('labels != -1')
    cross_not_nil = cross_df.query('labels != -1')
    assert bi_not_nil.shape[0] == cross_not_nil.shape[0]
    
    biencoder_accuracy_nn = bi_not_nil.query('`recall@` == 0').shape[0] / bi_not_nil.shape[0]
    biencoder_accuracy_nn_nil = bi_not_nil.query('labels == `best_candidate+nil`').shape[0] / bi_not_nil.shape[0]

    biencoder_recall10_nn = bi_not_nil.query('`recall@` >= 0 and `recall@` < 10').shape[0] / bi_not_nil.shape[0]
    biencoder_recall30_nn = bi_not_nil.query('`recall@` >= 0 and `recall@` < 30').shape[0] / bi_not_nil.shape[0]
    biencoder_recall100_nn = bi_not_nil.query('`recall@` >= 0 and `recall@` < 100').shape[0] / bi_not_nil.shape[0]
    
    overall_unnormalized_accuracy_nn = cross_not_nil.query('`recall@` == 0').shape[0] / cross_not_nil.shape[0]
    overall_unnormalized_accuracy_nn_nil = cross_not_nil.query('labels == `best_candidate+nil`').shape[0] / cross_not_nil.shape[0]
    
    cross_not_nil_fnd = cross_not_nil[cross_not_nil.apply(lambda x: x.labels in x.nns, axis=1)]
    
    crossencoder_normalized_accuracy_nn = cross_not_nil_fnd.query('`recall@` == 0').shape[0] / cross_not_nil_fnd.shape[0]
    crossencoder_normalized_accuracy_nn_nil = cross_not_nil_fnd.query('labels == `best_candidate+nil`').shape[0] / cross_not_nil_fnd.shape[0]


    report.loc[f'{d}-nil'] = {
            'bi_acc': biencoder_accuracy_nn,
            'bi_acc_nil': biencoder_accuracy_nn_nil,
            'bi_recall10': biencoder_recall10_nn,
            'bi_recall30': biencoder_recall30_nn,
            'bi_recall100': biencoder_recall100_nn,
            'cross_norm_acc': crossencoder_normalized_accuracy_nn,
            'overall_unn_acc': overall_unnormalized_accuracy_nn,
            'cross_norm_acc_nil': crossencoder_normalized_accuracy_nn_nil,
            'overall_unn_acc_nil': overall_unnormalized_accuracy_nn_nil,
            'nil_p_bi': -1,
            'nil_r_bi': -1,
            'nil_f1_bi': -1,
            'nil_acc_bi': -1,
            'nil_p_cross': -1,
            'nil_r_cross': -1,
            'nil_f1_cross': -1,
            'nil_acc_cross': -1
        }
    
    report.loc[f'{d}+nil'] = {
            'bi_acc': biencoder_accuracy,
            'bi_acc_nil': biencoder_accuracy_nil,
            'bi_recall10': biencoder_recall10,
            'bi_recall30': biencoder_recall30,
            'bi_recall100': biencoder_recall100,
            'cross_norm_acc': crossencoder_normalized_accuracy,
            'overall_unn_acc': overall_unnormalized_accuracy,
            'cross_norm_acc_nil': crossencoder_normalized_accuracy_nil,
            'overall_unn_acc_nil': overall_unnormalized_accuracy_nil,
            'nil_p_bi': nil_report_bi['0']['precision'],
            'nil_r_bi': nil_report_bi['0']['recall'],
            'nil_f1_bi': nil_report_bi['0']['f1-score'],
            'nil_acc_bi': nil_report_bi['accuracy'],
            'nil_p_cross': nil_report_cross['0']['precision'],
            'nil_r_cross': nil_report_cross['0']['recall'],
            'nil_f1_cross': nil_report_cross['0']['f1-score'],
            'nil_acc_cross': nil_report_cross['accuracy']
        }
report
```

```{python}
report.to_csv('report_20210922_nrl.csv')
```

```{python}
report_aug = pd.read_csv('report_20210922_nrl.csv', index_col=0)
report_aug
```

```{python}
report.columns
```

```{python}
# - nil
report.loc[[i for i in report.index if i.endswith("-nil")]][['bi_acc', 'bi_acc_nil', 'bi_recall10', 'bi_recall30', 'bi_recall100',
       'cross_norm_acc', 'overall_unn_acc', 'cross_norm_acc_nil',
       'overall_unn_acc_nil']]
```

```{python}
# + nil
report.loc[[i for i in report.index if i.endswith("+nil")]][['bi_acc', 'bi_acc_nil', 'bi_recall10', 'bi_recall30', 'bi_recall100',
       'cross_norm_acc', 'overall_unn_acc', 'cross_norm_acc_nil',
       'overall_unn_acc_nil']]
```

```{python}
# nil perf
report.loc[[i for i in report.index if i.endswith("+nil")]][['nil_p_bi', 'nil_r_bi', 'nil_f1_bi',
       'nil_acc_bi', 'nil_p_cross', 'nil_r_cross', 'nil_f1_cross',
       'nil_acc_cross']]
```

```{python}
y = (cross_df['labels'] != -1).astype(int).values
y
```

```{python}
y_pred= cross_df['nil_b'].values
y_pred
```

```{python}
from sklearn.metrics import classification_report
```

```{python}
(cross_df['labels'] == -1)
```

```{python}
print(classification_report(y, y_pred))
```

```{python}
d = classification_report(y, y_pred, output_dict=True)
d
```

```{python}
report.to_csv('report_20210921_svc_all_train_hard.csv')
```

```{python}
cross_df.query('labels == -1').shape
```

```{python}
nil_tp_cr = cross_df.query('labels != -1 and nil_b >= 0.5').shape[0]
nil_tn_cr = cross_df.query('labels == -1 and nil_b < 0.5').shape[0]
nil_fp_cr = cross_df.query('labels == -1 and nil_b >= 0.5').shape[0]
nil_fn_cr = cross_df.query('labels != -1 and nil_b < 0.5').shape[0]

nil_p_cross = nil_tp_cr / (nil_tp_cr + nil_fp_cr)
nil_r_cross = nil_tp_cr / (nil_tp_cr + nil_fn_cr)
nil_f1_cross = 2*nil_p_cross*nil_r_cross/(nil_p_cross+nil_r_cross)
nil_acc_cross = (nil_tp_cr+nil_tn_cr)/(nil_tp_cr+nil_tn_cr+nil_fp_cr+nil_fn_cr)
```

```{python}
nil_tn_cr / (nil_tn_cr + nil_fn_cr)
```

```{python}
cross_df['nil_b'] = cross_df['nil_score'].round().astype(int)
```

```{python}
scores_col = 'unsorted_scores'
#cross_df['best_candidate_05'] = cross_df.apply(lambda x: _best_candidate(x[scores_col], x.nns, x.nil_score, 0.5), axis=1)
cross_df['best_candidate_03'] = cross_df.apply(lambda x: _best_candidate(x[scores_col], x.nns, x.nil_score, 0.3), axis=1)
#cross_df['best_candidate_-1'] = cross_df.apply(lambda x: _best_candidate(x[scores_col], x.nns, x.nil_score, -1), axis=1)
```

```{python}
print("\t", "Acc", "NIL")
print('NIL@0.5', (cross_df['labels'] == cross_df['best_candidate_05']).sum() / cross_df.shape[0],
      cross_df.query('labels == -1 and best_candidate_05 == -1').count()[0] / cross_df.query('labels == -1').count()[0])
print('NIL@0.3', (cross_df['labels'] == cross_df['best_candidate_03']).sum() / cross_df.shape[0],
     cross_df.query('labels == -1 and best_candidate_03 == -1').count()[0] / cross_df.query('labels == -1').count()[0])
print('NIL@-1', (cross_df['labels'] == cross_df['best_candidate_-1']).sum() / cross_df.shape[0],
     cross_df.query('labels == -1 and "best_candidate_-1" == -1').count()[0] / cross_df.query('labels == -1').count()[0])
```

```{python}
(cross_df['labels'] == cross_df['best_candidate_05']).sum() / cross_df.shape[0]
```

```{python}
cross_df.query('labels == -1 and best_candidate_05 == -1').count()[0] / cross_df.query('labels == -1').count()[0]
```

```{python}
cross_df.query('nil_score > 0.8 and labels == best_candidate_05').count()[0] / \
    cross_df.query('nil_score > 0.8').count()[0]
```

```{python}
cross_df.query('nil_score < 0.2 and labels == best_candidate_05').count()[0] / \
    cross_df.query('nil_score < 0.2').count()[0]
```

```{python}
th = 0.2
cross_df.query(f'nil_score < {th}')['labels'].value_counts() / cross_df.query(f'nil_score < {th}').count()[0]
```

```{python}
cross_df[['labels', 'best_candidate_05']]
```

```{python}
# roc curve
fpr, tpr, thresholds = sklearn.metrics.roc_curve(cross_df.labels, cross_df.best_candidate_05)
pd.DataFrame({
    'tpr': tpr,
    'fpr': fpr,
    'thresholds': thresholds
}).plot(x='fpr', y='tpr', title='roc curve')
plt.show()
```

```{python}
import sklearn
```

```{python}
df = cross_df.query('nil_score <= 0.2 or nil_score >= 0.8')
target = df['labels']
pred = df['best_candidate_05']
sklearn.metrics.accuracy_score(target, pred)
```

```{python}
pd.read_csv('report.csv', index_col=0)
```

```{python}
datasets
```

```{python}
d = 'AIDA-YAGO2_testa_scores'

bi_path = f'{d}_bi.jsonl'
cross_path = f'{d}_cross.jsonl'
bi_stats, _, combined_stats, bi_df, cross_df = _load_scores(bi_path, cross_path, basepath)
```

```{python}
bi_stats
```

```{python}
assert bi_df.shape[0] == cross_df.shape[0]

bi_df['recall@'] = bi_df.apply(lambda x: _eval_line(x, 'scores'), axis=1)
bi_df['best_candidate'] = bi_df.apply(lambda x: _best_candidate(x['scores'], x.nns), axis=1)

bi_df['nil_p'] = nilclf_bi(bi_stats[features_bi].values)
bi_df['nil_b'] = np.round(bi_df['nil_p'])
bi_df['best_candidate+nil'] = bi_df.apply(
    lambda x: _best_candidate(x['scores'], x.nns, x['nil_b']), axis=1)

cross_df['recall@'] = cross_df.apply(lambda x: _eval_line(x, 'unsorted_scores'), axis=1)
cross_df['best_candidate'] = cross_df.apply(lambda x: _best_candidate(x['unsorted_scores'], x.nns), axis=1)

cross_df['nil_p'] = nilclf(combined_stats[features].values)
cross_df['nil_b'] = np.round(cross_df['nil_p'])

cross_df['best_candidate+nil'] = cross_df.apply(
    lambda x: _best_candidate(x['unsorted_scores'], x.nns, x['nil_b']), axis=1)
```

```{python}
['../data/BLINK_benchmark_with_NIL/{}.jsonl'.format(d) for d in datasets]
```

```{python}
datasets
```

```{python}
base_path = '../data/BLINK_benchmark_with_NIL'
save_path = './dataset_and_preds/'
if not os.path.isdir(save_path):
    os.mkdir(save_path)
for d in datasets:
    bi_path = f'{d}_bi.jsonl'
    cross_path = f'{d}_cross.jsonl'
    bi_stats, _, combined_stats, bi_df, cross_df = _load_scores(bi_path, cross_path, basepath)

    assert bi_df.shape[0] == cross_df.shape[0]

    bi_df['recall@'] = bi_df.apply(lambda x: _eval_line(x, 'scores'), axis=1)
    bi_df['best_candidate'] = bi_df.apply(lambda x: _best_candidate(x['scores'], x.nns), axis=1)

    bi_df['nil_p'] = nilclf_bi(bi_stats[features_bi].values)
    bi_df['nil_b'] = np.round(bi_df['nil_p'])

    bi_df['best_candidate+nil'] = bi_df.apply(
        lambda x: _best_candidate(x['scores'], x.nns, x['nil_b']), axis=1)

    cross_df['recall@'] = cross_df.apply(lambda x: _eval_line(x, 'unsorted_scores'), axis=1)
    cross_df['best_candidate'] = cross_df.apply(lambda x: _best_candidate(x['unsorted_scores'], x.nns), axis=1)

    cross_df['nil_p'] = nilclf(combined_stats[features].values)
    cross_df['nil_b'] = np.round(cross_df['nil_p'])

    cross_df['best_candidate+nil'] = cross_df.apply(
        lambda x: _best_candidate(x['unsorted_scores'], x.nns, x['nil_b']), axis=1)
    
    d_source = pd.read_json('{}.jsonl'.format(os.path.join(base_path, d.replace('_scores',''))), lines=True)
    d_source[['bi_'+c for c in bi_df.columns]] = bi_df
    d_source[['cross_'+c for c in cross_df.columns]] = cross_df
    
    dest = os.path.join(save_path, d.replace('_scores', ''))+ '.csv'
    d_source.to_csv(dest)
```

```{python}
base_path = '../data/BLINK_benchmark_with_NIL'
print('{}.jsonl'.format(os.path.join(base_path, d.replace('_scores',''))))
os.path.isfile('{}.jsonl'.format(os.path.join(base_path, d.replace('_scores',''))))
```

```{python}
d_source = pd.read_json('../data/BLINK_benchmark_with_NIL/AIDA-YAGO2_testa.jsonl', lines=True)
d_source
```

```{python}
['bi_'+c for c in bi_df.columns]
```

```{python}
d_source[['bi_'+c for c in bi_df.columns]] = bi_df
```

```{python}
d_source[['cross_'+c for c in cross_df.columns]] = cross_df
```

```{python}
d_source.columns
```

```{python}
d_source.query('cross_best_candidate != cross_labels').count()[0] / d_source.shape[0]
```

```{python}
d_source.query('cross_best_candidate != cross_labels and cross_labels != -1').count()[0]  / d_source.shape[0]
```

```{python}
d_source.query('`cross_best_candidate+nil` != cross_labels').count()[0] / d_source.shape[0]
```

```{python}
import pickle
with open('entity_ids/wikipedia_id2local_id.pickle', 'rb') as fd:
    wikipedia_id2local_id = pickle.load(fd)
    pass
with open('entity_ids/title2id.pickle', 'rb') as fd:
    #title2id = pickle.load(fd)
    pass
with open('entity_ids/local_id2wikipedia_id.pickle', 'rb') as fd:
    local_id2wikipedia_id = pickle.load(fd)
    pass
with open('entity_ids/id2text.pickle', 'rb') as fd:
    #id2text = pickle.load(fd)
    pass
with open('entity_ids/id2title.pickle', 'rb') as fd:
    #id2title = pickle.load(fd)
    pass
```

```{python}
import redis
```

```{python}
r_id2title = redis.Redis(host='localhost', port=6379, db=0)
r_id2text = redis.Redis(host='localhost', port=6379, db=1)
r_local_id2wikipedia_id = redis.Redis(host='localhost', port=6379, db=2)
r_title2id = redis.Redis(host='localhost', port=6379, db=3)
r_wikipedia_id2local_id = redis.Redis(host='localhost', port=6379, db=4)
# db=0 id2title
# 1 id2text
```

```{python}
#for k,v in id2title.items():
#    r_id2title.set(k, v)
```

```{python}
for k,v in id2text.items():
    r_id2text.set(k, v)
```

```{python}
#for k,v in local_id2wikipedia_id.items():
#    r_local_id2wikipedia_id.set(k, v)
```

```{python}
#for k,v in title2id.items():
#    r_title2id.set(k, v)
```

```{python}
#for k,v in wikipedia_id2local_id.items():
#    r_wikipedia_id2local_id.set(k, v)
```

```{python}
pd.options.display.max_colwidth = 100
def myf(x, nwords=20):
    x['cross_labels'] = id2title[x['cross_labels']]
    x['cross_best_candidate'] = id2title[x['cross_best_candidate']]
    x['context_left'] = ' '.join(x['context_left'].split(' ')[-nwords:])
    x['context_right'] = ' '.join(x['context_right'].split(' ')[:nwords])
    x['text'] = '{} [{}] {}'.format(x['context_left'], x['mention'], x['context_right'])
    return x
```

```{python}
# cross errors
df = d_source.query('cross_best_candidate != cross_labels and cross_labels != -1')\
    .apply(myf, axis=1)
```

```{python}
d_source.columns
```

```{python}
# cross ok nil errors (false nil)
print(d_source.query('cross_best_candidate == cross_labels and `cross_best_candidate+nil` == -1').count()[0]\
      / d_source.shape[0])
print(d_source.query('cross_labels == -1 and `cross_best_candidate+nil` != -1').count()[0]\
      / d_source.shape[0])
print(d_source.query('cross_labels == -1 and `cross_best_candidate+nil` == -1').count()[0]\
      / d_source.shape[0])
df = d_source.query('cross_best_candidate == cross_labels and `cross_best_candidate+nil` == -1')\
    .apply(myf, axis=1)
```

```{python}
for i, row in df.iterrows():
    print('[{}]\t[{}] - {}/{:.2f}'.format(row['Wikipedia_title'], row['cross_best_candidate'],
                                      row['cross_nil_b'], row['cross_nil_p']))
    print(row['text'])
    print()
```

```{python}
df = d_source.query('cross_best_candidate != cross_labels and cross_labels != -1')\
    [['context_left', 'mention','context_right', 'Wikipedia_title', 'cross_labels', 'cross_best_candidate']]
```

```{python}
for i in df.iterrows():
    print(i)
```

```{python}
os.path.join('a','b','.csv')
```

```{python}
os.listdir('./dataset_and_preds/')
```

```{python}
dpreds_path = ['msnbc_questions.csv',
 'AIDA-YAGO2_train.csv',
 'clueweb_questions.csv',
 'AIDA-YAGO2_testa.csv',
 'AIDA-YAGO2_testb.csv',
 'wnedwiki_questions.csv',
 'aquaint_questions.csv',
 'ace2004_questions.csv']
dpreds_path = [os.path.join('dataset_and_preds', p) for p in dpreds_path]
```

```{python}
whole = pd.DataFrame()
for dp in dpreds_path:
    current = pd.read_csv(dp, index_col=0)
    current['src'] = dp
    whole = pd.concat([whole, current])
```

```{python}
# shuffle
whole = whole.sample(frac=1)
```

```{python}
whole.columns
```

```{python}
whole.query('cross_best_candidate != cross_labels and cross_labels != -1')
```

```{python}
pd.options.display.max_colwidth = 100
def myf(x, nwords=20):
    x['cross_labels_title'] = id2title[x['cross_labels']] if x['cross_labels'] != -1 else 'NIL'
    x['cross_best_candidate_title'] = id2title[x['cross_best_candidate']]
    x['cross_best_candidate+nil_title'] = id2title[x['cross_best_candidate+nil']] if x['cross_best_candidate+nil'] != -1 else 'NIL'
    
    x['bi_labels_title'] = id2title[x['bi_labels']] if x['bi_labels'] != -1 else 'NIL'
    x['bi_best_candidate_title'] = id2title[x['bi_best_candidate']]
    x['bi_best_candidate+nil_title'] = id2title[x['bi_best_candidate+nil']] if x['bi_best_candidate+nil'] != -1 else 'NIL'

    return x
```

```{python}
whole2['cross_best_candidate+nil']
```

```{python}
whole2 = whole.apply(myf, axis=1)
```

```{python}
whole2.to_csv('dataset_and_preds/whole2.csv')
```

```{python}
whole2 = pd.read_csv('dataset_and_preds/whole2.csv', index_col=0)
```

```{python}
cross_errors = whole2.query('cross_best_candidate != cross_labels and cross_labels != -1')
```

```{python}
#cross_errors.to_csv('dataset_and_preds/cross_errors.csv')
```

```{python}
wrongly_identified_as_nil = whole2.query('cross_best_candidate == cross_labels and `cross_best_candidate+nil` == -1')
#wrongly_identified_as_nil.to_csv('dataset_and_preds/wrongly_identified_as_nil.csv')
```

```{python}
wrongly_identified_as_not_nil = whole2.query('cross_labels == -1 and `cross_best_candidate+nil` != -1')
#wrongly_identified_as_not_nil.to_csv('dataset_and_preds/wrongly_identified_as_not_nil.csv')
```

```{python}
bi_errors = whole2.query('bi_best_candidate != bi_labels and bi_labels != -1')
#bi_errors.to_csv('dataset_and_preds/bi_errors.csv')
```

```{python}
wrongly_identified_as_nil_bi = whole2.query('bi_best_candidate == bi_labels and `bi_best_candidate+nil` == -1')
#wrongly_identified_as_nil_bi.to_csv('dataset_and_preds/wrongly_identified_as_nil_bi.csv')
```

```{python}
wrongly_identified_as_not_nil_bi = whole2.query('bi_labels == -1 and `bi_best_candidate+nil` != -1')
#wrongly_identified_as_not_nil_bi.to_csv('dataset_and_preds/wrongly_identified_as_not_nil_bi.csv')
```

```{python}
wrongly_identified_as_not_nil.query('Wikipedia_title == `cross_best_candidate+nil_title`')[['Wikipedia_title', 'cross_best_candidate+nil_title']]
```

```{python}
wrongly_identified_as_not_nil.query('Wikipedia_title == `cross_best_candidate+nil_title`').count()[0] / \
    wrongly_identified_as_not_nil.shape[0]
```

```{python}
wrongly_identified_as_not_nil
```

```{python}
import json
```

```{python}
def show_errors(df, nw=20):
    for i, row in df.iterrows():

        if isinstance(row['bi_scores'], str):
            bi_scores = json.loads(row['bi_scores'])
        else:
            bi_scores = row['bi_scores']

        bi_best_score = max(bi_scores)

        if isinstance(row['cross_unsorted_scores'], str):
            cross_scores = json.loads(row['cross_unsorted_scores'])
        else:
            cross_scores = row['cross_unsorted_scores']

        cross_best_score = max(cross_scores)

        print('<{}> [{}] [{}] [{}] - {}/{:.2f}/{:.2f} - {:.2f}|{:.2f} - {}'.format(
            row['Wikipedia_title'],
            row['cross_labels_title'],
            row['cross_best_candidate_title'],
            row['cross_best_candidate+nil_title'],
            row['cross_nil_b'],
            row['cross_nil_p'],
            row['bi_nil_p'],
            bi_best_score,
            cross_best_score,
            row['mention']

        ))


        left = ' '.join(str(row['context_left']).split(' ')[-nw:])
        right = ' '.join(str(row['context_right']).split(' ')[:nw])
        print('{} **[**{}**]** {}'.format(left, row['mention'], right))
        print()
```

```{python}
df = wrongly_identified_as_not_nil.query('Wikipedia_title != `cross_best_candidate+nil_title`')
show_errors(df)
```

```{python}
df = wrongly_identified_as_nil
# sembra che quando le menzioni non matchano esattamente lo score di blink è un po' basso
show_errors(df)
```

```{python}
wrongly_identified_as_nil.count()[0]/whole2.shape[0]
```

```{python}
df = cross_errors.query('Wikipedia_title != `cross_best_candidate+nil_title`')
show_errors(df)
```

```{python}
df = bi_errors.query('Wikipedia_title != `bi_best_candidate+nil_title`')

show_errors(df)
```

```{python}
df = wrongly_identified_as_not_nil_bi.query('Wikipedia_title != `cross_best_candidate+nil_title`')

show_errors(df)
```

```{python}
df = wrongly_identified_as_nil_bi
# sembra che quando le menzioni non matchano esattamente lo score di blink è un po' basso

show_errors(df)
```

```{python}
wrongly_identified_as_nil_bi.query('`cross_best_candidate+nil` == cross_labels').count()[0]/ \
    wrongly_identified_as_nil_bi.shape[0]
# cross corrects 43% samples wrongly identified as nil
```

```{python}
wrongly_identified_as_not_nil_bi_correct = wrongly_identified_as_not_nil_bi.query('Wikipedia_title != `cross_best_candidate+nil_title`')
```

```{python}
wrongly_identified_as_not_nil_bi_correct.query('`cross_best_candidate+nil` == cross_labels').count()[0] / \
    wrongly_identified_as_not_nil_bi_correct.shape[0]
# cross corrects 8% of samples identified as not nil while they should have been nil
```

```{python}
wrongly_identified_as_not_nil_correct  = wrongly_identified_as_not_nil.query('Wikipedia_title != `cross_best_candidate+nil_title`')
```

```{python}
wrongly_identified_as_not_nil_correct.query('`bi_best_candidate+nil` == bi_labels').count()[0] / \
    wrongly_identified_as_not_nil_correct.shape[0]
# bi corrects 18% of samples identified as not nil while they should have been nil
# # ??
```

```{python}
# investigare su id di wikipedia. aida magari è su un dump diverso
```

```{python}
report = pd.read_csv('report_20200916.csv', index_col=0)
```

```{python}
report
```

```{python}
fn+tp
```

```{python}
(whole2['cross_labels'] == -1).sum()
```

```{python}
whole2.query('cross_labels == -1').count()[0]
```

```{python}
tp = whole2.query('`cross_best_candidate+nil` == -1 and cross_labels == -1').shape[0]
fp = whole2.query('`cross_best_candidate+nil` == -1 and cross_labels != -1').shape[0]
fn = whole2.query('`cross_best_candidate+nil` != -1 and cross_labels == -1').shape[0]
tot = (whole2['cross_labels'] == -1).sum()
assert tp+fn == tot
p = tp / (tp+fp)
r = tp / tot
print(p, r)
```

```{python}
(whole2['cross_labels'] == -1).sum()
```

# oracolo

```{python}
whole2.columns
```

```{python}
whole2['src'].value_counts()
```

```{python}
print(whole2.query('cross_labels == `bi_best_candidate+nil`').shape[0] / whole2.shape[0])
whole2.query('cross_labels == `bi_best_candidate+nil` or Wikipedia_title == `bi_best_candidate+nil_title`').shape[0] / whole2.shape[0]
```

```{python}
print(whole2.query('cross_labels == `cross_best_candidate+nil`').shape[0] / whole2.shape[0])
whole2.query('cross_labels == `cross_best_candidate+nil` or Wikipedia_title == `cross_best_candidate+nil_title`').shape[0] / whole2.shape[0]
```

```{python}
whole2.query('cross_labels == `bi_best_candidate+nil`').shape[0] / whole2.shape[0]
```

```{python}
th = 0.6
tl = 0.3
subs = whole2.query(f'cross_nil_p <= {tl} or cross_nil_p >= {th}')
print('requests to human', 1 - subs.shape[0]/whole2.shape[0])
print(subs.query('cross_labels == `cross_best_candidate+nil` or Wikipedia_title == `cross_best_candidate+nil_title`').shape[0] / subs.shape[0])
```

```{python}
th = 0.6
tl = 0.3
subs = whole2.query(f'bi_nil_p <= {tl} or bi_nil_p >= {th}')
print('requests to human', 1 - subs.shape[0]/whole2.shape[0])
print(subs.query('cross_labels == `bi_best_candidate+nil` or Wikipedia_title == `bi_best_candidate+nil_title`').shape[0] / subs.shape[0])
```

```{python}
aida_traina = whole2.query('src == "dataset_and_preds/AIDA-YAGO2_testa.csv"')
aida_traina
```

```{python}
aida_traina.query('cross_labels == `bi_best_candidate`').shape[0] / aida_traina.shape[0]
```

```{python}
aida_traina.query('cross_labels == `cross_best_candidate`').shape[0] / aida_traina.shape[0]
```

```{python}
aida_traina.query('cross_labels == `bi_best_candidate+nil`').shape[0] / aida_traina.shape[0]
```

```{python}
aida_traina.query('cross_labels == `cross_best_candidate+nil`').shape[0] / aida_traina.shape[0]
```

```{python}
(whole2['cross_labels'] != -1).astype(int).plot(kind='density')
```

```{python}
whole2['cross_nil_p'].plot(kind='density')
```

```{python}
whole2['bi_nil_p'].plot(kind='density')
```

```{python}
print(wrongly_identified_as_nil['cross_nil_p'].describe())
wrongly_identified_as_nil['cross_nil_p'].plot(kind='density')
```

```{python}
(wrongly_identified_as_nil['cross_nil_p'] <= 0.25).sum() / wrongly_identified_as_nil.shape[0]
```

```{python}
print(wrongly_identified_as_nil_bi['bi_nil_p'].describe())
wrongly_identified_as_nil_bi['bi_nil_p'].plot(kind='density')
```

```{python}
(wrongly_identified_as_nil_bi['bi_nil_p'] <= 0.25).sum() / wrongly_identified_as_nil_bi.shape[0]
```

# whole with stats

```{python}
_test = np.array(np.array(list(range(0,100,5))[:10]))
_test
```

```{python}
_test[np.argsort(_test)[::-1]]
```

```{python}
def _bc_get_stats(x, remove_correct = False, scores_col='scores', nns_col='nns', labels_col='labels', top_k=100):
    scores = x[scores_col]
    nns = x[nns_col]
    if isinstance(scores, str):
        scores = np.array(json.loads(scores))
    if isinstance(nns, str):
        nns = np.array(json.loads(nns))

    assert len(scores) == len(nns)
    scores = scores.copy()
    
    sort_scores_i = np.argsort(scores)[::-1]
    scores = scores[sort_scores_i][:top_k]
    
    nns = nns.copy()
    nns = nns[sort_scores_i][:top_k]
    
    correct = None
    if x[labels_col] in nns:
        # found correct entity
        i_correct = list(nns).index(x[labels_col])
        correct = scores[i_correct]

    _stats = {
        "correct": correct,
        "max": max(scores),
        "second": sorted(scores, reverse=True)[1],
        "min": min(scores),
        "mean": statistics.mean(scores),
        "median": statistics.median(scores),
        "stdev": statistics.stdev(scores)
    }
    return _stats
```

```{python}
whole2.columns
```

```{python}
isinstance(whole2.iloc[0]['cross_unsorted_scores'], str)
```

```{python}
whole2.iloc[0:10]['cross_unsorted_scores']
```

```{python}
whole2.iloc[0:10].apply(
    lambda x: _bc_get_stats(x, 
                            scores_col='cross_unsorted_scores',
                            nns_col='cross_nns',
                            labels_col='cross_labels',
                            top_k=10
                           ),
    axis=1, result_type='expand')
```

```{python}
temp = pd.DataFrame()
cols = ['correct', 'max', 'second', 'min', 'median', 'stdev']
cols = ['cross_10_'+c for c in cols]
```

```{python}
cross_stats100 = whole2.apply(
    lambda x: _bc_get_stats(x, 
                            scores_col='cross_unsorted_scores',
                            nns_col='cross_nns',
                            labels_col='cross_labels',
                            top_k=100
                           ),
    axis=1, result_type='expand')
```

```{python}
whole2[['cross_stats_100_'+c for c in cross_stats100.columns]] = cross_stats100
```

```{python}
cross_stats10 = whole2.apply(
    lambda x: _bc_get_stats(x, 
                            scores_col='cross_unsorted_scores',
                            nns_col='cross_nns',
                            labels_col='cross_labels',
                            top_k=10
                           ),
    axis=1, result_type='expand')
```

```{python}
whole2[['cross_stats_10_'+c for c in cross_stats10.columns]] = cross_stats10
```

```{python}
bi_stats100 = whole2.apply(
    lambda x: _bc_get_stats(x, 
                            scores_col='bi_scores',
                            nns_col='bi_nns',
                            labels_col='bi_labels',
                            top_k=100
                           ),
    axis=1, result_type='expand')
```

```{python}
whole2[['bi_stats_100_'+c for c in bi_stats100.columns]] = bi_stats100
```

```{python}
bi_stats10 = whole2.apply(
    lambda x: _bc_get_stats(x, 
                            scores_col='bi_scores',
                            nns_col='bi_nns',
                            labels_col='bi_labels',
                            top_k=10
                           ),
    axis=1, result_type='expand')
```

```{python}
whole2[['bi_stats_10_'+c for c in bi_stats10.columns]] = bi_stats10
```

```{python}
whole2.columns
```

```{python}
whole2.to_csv('whole2_with_100_10_stats.csv')
```

```{python}
#whole2 = pd.read_csv('whole2_with_100_10_stats.csv', index_col=0)
whole2= pd.read_csv('whole_df5_dists_topk.csv', index_col=0)
whole2.head()
```

```{python}
nns, lab, scores = whole2.iloc[1:2][['cross_nns', 'cross_labels', 'cross_unsorted_scores']].values[0]
```

```{python}
list(nns).index(lab)
```

```{python}
nns = np.array(json.loads(nns))
nns
```

```{python}
scores = np.array(json.loads(scores))
scores
```

```{python}
import textdistance

textdistance.hamming('test', 'text')
# 1

textdistance.hamming.distance('test', 'text')
# 1

textdistance.hamming.similarity('test', 'text')
# 3

textdistance.hamming.normalized_distance('test', 'text')
# 0.25

textdistance.hamming.normalized_similarity('test', 'text')
# 0.75

textdistance.Hamming(qval=2).distance('test', 'text')
```

```{python}
textdistance.jaccard.normalized_similarity('Christiano', 'ronaldo')
```

```{python}
textdistance.jaccard.normalized_similarity('Christiano', 'chsristiano ronaldo')
```

```{python}
textdistance.jaccard.normalized_similarity('Riccardo Pozzi', 'Riccardo Pozzo')
```

```{python}
textdistance.damerau_levenshtein.normalized_similarity('Riccardo Pozzi', 'Riccardo Pozzo')
```

```{python}
def _text_distance(x, a='mention', b='cross_labels_title'):
    token = textdistance.jaccard.normalized_similarity(x[a], x[b])
    edit = textdistance.damerau_levenshtein.normalized_similarity(x[a], x[b])
    r = {
        'jaccard': token,
        'damerau_levenshtein': edit
    }
    return r
```

```{python}
dsNN = wrongly_identified_as_nil.apply(_text_distance, axis=1, result_type='expand')
```

```{python}
dsNN.describe()
```

```{python}
dsNN.plot(kind='density')
```

```{python}
whole2.columns
```

```{python}
cross_text_ds = whole2.apply(
    lambda x: _text_distance(x, a='mention', b='cross_best_candidate_title'),
    axis=1, result_type='expand')
whole2[['cross_'+c for c in cross_text_ds.columns]] = cross_text_ds
```

```{python}
bi_text_ds = whole2.apply(
    lambda x: _text_distance(x, a='mention', b='bi_best_candidate_title'),
    axis=1, result_type='expand')
whole2[['bi_'+c for c in bi_text_ds.columns]] = bi_text_ds
```

```{python}
whole2.columns
```

```{python}
whole2[['bi_jaccard', 'bi_damerau_levenshtein', 'cross_jaccard', 'cross_damerau_levenshtein']].plot(kind='density')
```

```{python}
whole2.to_csv('whole2_with_100_10_stats_ds.csv')
```

```{python}
ds.describe()
```

```{python}
ds
```

```{python}
ds = whole2.query('cross_labels == -1 and `cross_best_candidate+nil` != -1').apply(
    lambda x: _text_distance(x, a='mention', b='cross_best_candidate+nil_title'),
    axis=1, result_type='expand')
```

```{python}
ds.plot(kind='density')
```

```{python}
ds.plot(kind='density')
```

```{python}
ds = whole2.query('cross_labels == `cross_best_candidate+nil`').apply(
    lambda x: _text_distance(x, a='mention', b='cross_best_candidate+nil_title'),
    axis=1, result_type='expand')
ds.plot(kind='density')
```

```{python}
ds = whole2.query('cross_labels != -1 and `cross_best_candidate+nil` == -1').apply(
    lambda x: _text_distance(x, a='mention', b='cross_best_candidate_title'),
    axis=1, result_type='expand')
ds.plot(kind='density')
```

```{python}
ds = whole2.query('cross_labels == -1 and `cross_best_candidate+nil` != -1').apply(
    lambda x: _text_distance(x, a='mention', b='cross_labels_title'),
    axis=1, result_type='expand')
ds.plot(kind='density')
```

```{python}
del whole2
```

# whole5

```{python}
import pandas as pd
whole5 = pd.read_csv('whole6.csv', index_col=0)
```

```{python}
import pickle
```

```{python}
#with open('train_new_out/aida_under_all10_ner_wiki_but_stats_model.pickle', 'rb') as fd:
with open('train_new_out/aida_under_all10_max_stdev4_hamming_ner_wiki_model.pickle', 'rb') as fd:
    clf = pickle.load(fd)
```

```{python}
#with open('train_new_out/aida_under_bi_max_dst_ner_wiki_model.pickle', 'rb') as fd:
with open('train_new_out/aida_under_bi_max_ner_wiki_stdev4_model.pickle', 'rb') as fd:
    clf_bi = pickle.load(fd)
```

```{python}
features=  [
                'cross_stats_10_max',
                'bi_stats_10_max',
                'cross_stats_4_stdev',
                'bi_stats_4_stdev',    
                'cross_hamming',
                'bi_hamming',
                #'cross_sim_jaccard',
                #'bi_sim_jaccard',
                'ner_per',
                'ner_loc',
                'ner_org',
                'ner_misc',
                'wiki_per_cross',
                'wiki_loc_cross',
                'wiki_org_cross',
                'wiki_misc_cross'
            ]
features_bi=  [
                'bi_stats_10_max',
                'bi_stats_4_stdev',
                #'bi_sim_jaccard',
                'ner_per',
                'ner_loc',
                'ner_org',
                'ner_misc',
                'wiki_per_bi',
                'wiki_loc_bi',
                'wiki_org_bi',
                'wiki_misc_bi',]
```

```{python}
import numpy as np, json
```

```{python}
aida5 = whole5[whole5['src'].apply(lambda x: 'aida' in x.lower())].copy()
```

```{python}
aida5['y_pred_cross'] = np.array([i for _,i in clf.predict_proba(aida5[features])])
```

```{python}
aida5['y_pred_bi'] = np.array([i for _,i in clf_bi.predict_proba(aida5[features_bi])])
```

```{python}
cross_errors = aida5.query('cross_best_candidate != cross_labels and cross_labels != -1')
```

```{python}
def show_errors(df, nw=20):
    print('<{wiki_title}> [{cross_label_title}] [{cross_best_candidate_title}]'
              '[{cross_best_cand_or_nil}] - {y_cross}/{y_bi} - {cross_best_score}|{bi_best_score} - {mention}')
    print()
    for i, row in df.iterrows():

        if isinstance(row['bi_scores'], str):
            bi_scores = json.loads(row['bi_scores'])
        else:
            bi_scores = row['bi_scores']

        bi_best_score = max(bi_scores)

        if isinstance(row['cross_unsorted_scores'], str):
            cross_scores = json.loads(row['cross_unsorted_scores'])
        else:
            cross_scores = row['cross_unsorted_scores']

        cross_best_score = max(cross_scores)
        
        best_cand_nil = row['cross_best_candidate_title'] if row['y_pred_cross'] >= 0.5 else 'NIL'
        
        wiki_title=row['Wikipedia_title']
        cross_label_title = row['cross_labels_title']
        cross_lab = row['cross_labels']
        cross_best_candidate = row['cross_best_candidate']
        cross_best_candidate_title = row['cross_best_candidate_title']

        print(f'<{wiki_title}> [{cross_label_title} ({cross_lab})] [{cross_best_candidate_title} ({cross_best_candidate})]'
              '[{}] - {:.2f}/{:.2f} - {:.2f}|{:.2f} - {}'.format(
            
            best_cand_nil,
            row['y_pred_cross'],
            row['y_pred_bi'],
            cross_best_score,
            bi_best_score,
            row['mention']

        ))


        left = ' '.join(str(row['context_left']).split(' ')[-nw:])
        right = ' '.join(str(row['context_right']).split(' ')[:nw])
        print('{} **[**{}**]** {}'.format(left, row['mention'], right))
        print()
```

```{python}
show_errors(cross_errors)
```

```{python}
wrongly_identified_as_nil = aida5.query('cross_best_candidate == cross_labels and y_pred_cross <0.5')
```

```{python}
wrongly_identified_as_nil_bi = aida5.query('bi_best_candidate == bi_labels and y_pred_bi <0.5')
print(wrongly_identified_as_nil_bi.shape[0]/aida5.shape[0])
print(aida5.query('bi_best_candidate == bi_labels and y_pred_bi <0.25').shape[0]/aida5.shape[0])
```

```{python}
# 37% corrected by cross
wrongly_identified_as_nil_bi.query('y_pred_cross >=0.5').shape[0]/wrongly_identified_as_nil_bi.shape[0]
```

```{python}
# 17% corrected by cross
wrongly_identified_as_nil.query('y_pred_bi >=0.5').shape[0]/wrongly_identified_as_nil.shape[0]
```

```{python}
aida5.query('cross_best_candidate == cross_labels and y_pred_cross <0.5').shape[0]/aida5.shape[0]
```

```{python}
aida5.query('cross_best_candidate == cross_labels and y_pred_cross <0.25').shape[0]/aida5.shape[0]
```

```{python}
aida5.query('cross_best_candidate == cross_labels and y_pred_cross <0.5')['y_pred_cross'].plot(kind='kde')
```

```{python}
aida5.query('cross_best_candidate == cross_labels and y_pred_cross <0.5')['y_pred_cross'].describe()
```

```{python}
show_errors(wrongly_identified_as_nil)
```

```{python}
show_errors(wrongly_identified_as_nil_bi)
```

```{python}
wrongly_identified_as_not_nil = aida5.query(
    'cross_best_candidate != cross_labels and cross_labels != -1 and y_pred_cross >=0.5')
```

```{python}
show_errors(wrongly_identified_as_not_nil)
```

```{python}
import textdistance
```

```{python}
textdistance.jaccard.similarity('Darren Bragg', 'Bragg')
```

```{python}
textdistance.jaccard.normalized_similarity('PT Timor Putra Nasional', 'East Timor')
```

```{python}

```

```{python}
whole5[['mention', 'cross_best_candidate_title', 'cross_sim_jaccard', 'bi_best_candidate_title', 'bi_sim_jaccard']]
```

```{python}
[c for c in whole5.columns if 'hamming' in c]
```

```{python}
whole5['bi_sim_jaccard'] = whole5.apply(
    lambda x: textdistance.jaccard.normalized_similarity(x.mention, x.bi_best_candidate_title), axis=1)
```

```{python}
whole5['cross_sim_jaccard'] = whole5.apply(
    lambda x: textdistance.jaccard.normalized_similarity(x.mention, x.cross_best_candidate_title), axis=1)
```

```{python}
#whole5.to_csv('whole6.csv')
```

# stats

```{python}
import pandas as pd
from matplotlib import pyplot
```

```{python}
[c for c in whole5.columns if 'mean' in c.lower()]
```

```{python}
# only the mentions we found the correct entity
# sort and remove na
cross_stats_s = whole5[whole5['cross_stats_10_correct'].notna()].sort_values('cross_stats_10_correct')
# plot
pyplot.scatter(x=range(cross_stats_s.shape[0]), y=cross_stats_s['cross_stats_10_max'], s=0.01, c='green')
pyplot.scatter(x=range(cross_stats_s.shape[0]), y=cross_stats_s['cross_stats_10_mean'], s=0.01, c='orange')
pyplot.scatter(x=range(cross_stats_s.shape[0]), y=cross_stats_s['cross_stats_10_correct'], s=0.01, c='blue')
#pyplot.scatter(x=range(cross_stats_s.shape[0]), y=cross_stats_s['median'], s=0.01, c='red')
pyplot.legend(
    labels = ['correct', 'max', 'mean'],
    labelcolor = ['blue', 'green', 'orange']
) 
```

```{python}
# only the mentions we found the correct entity # unsorted
# sort and remove na
cross_stats_u = whole5[whole5['cross_stats_10_correct'].notna()]
# plot
pyplot.scatter(x=range(cross_stats_u.shape[0]), y=cross_stats_u['cross_stats_10_max'], s=0.1, c='red')
pyplot.scatter(x=range(cross_stats_u.shape[0]), y=cross_stats_u['cross_stats_10_mean'], s=0.1, c='yellow')
pyplot.scatter(x=range(cross_stats_u.shape[0]), y=cross_stats_u['cross_stats_10_correct'], s=0.02, c='blue')
#pyplot.scatter(x=range(cross_stats_u.shape[0]), y=cross_stats_u['median'], s=0.01, c='orange')
pyplot.legend(
    labels = ['correct', 'max', 'mean'],
    labelcolor = ['green', 'red', 'yellow']
) 
```

```{python}
# the mentions we didn't find the correct entity
# sort and remove na
cross_stats_na = whole5[whole5['cross_stats_10_correct'].isna()]
# plot
#pyplot.scatter(x=range(cross_stats_na.shape[0]), y=cross_stats_na['cross_stats_10_correct'], s=1, c='blue')
pyplot.scatter(x=range(cross_stats_na.shape[0]), y=cross_stats_na['cross_stats_10_max'], s=1, c='green')
pyplot.scatter(x=range(cross_stats_na.shape[0]), y=cross_stats_na['cross_stats_10_mean'], s=1, c='orange')
pyplot.scatter(x=range(cross_stats_na.shape[0]), y=cross_stats_na['cross_stats_10_median'], s=1, c='red')
pyplot.scatter(x=range(cross_stats_na.shape[0]), y=cross_stats_na['cross_stats_10_min'], s=1, c='blue')
pyplot.legend(
    labels = ['max', 'mean', 'median', 'min'],
    labelcolor = ['green', 'orange', 'red', 'blue']
)
```

```{python}
# correct found
whole5[whole5['cross_stats_10_correct'].notna()][
    ['cross_stats_10_correct', 'cross_stats_10_max',
     'cross_stats_10_mean', 'cross_stats_10_median']].plot(kind='density')
```

```{python}
# correct not found
whole5[whole5['cross_stats_10_correct'].isna()][
    ['cross_stats_10_max',
     'cross_stats_10_mean', 'cross_stats_10_median']].plot(kind='density')
```

# avg ner

```{python}
[c for c in whole5.columns if 'ner' in c.lower()]
```

```{python}
whole5.loc[[1]]
```

```{python}
whole5['label_id']
```

```{python}
aida = whole5#[whole5['src'].apply(lambda x: 'AIDA' in x)]
```

```{python}
#del whole5
```

```{python}
[c for c in aida.columns if 'ner' in c]
```

```{python}
aida = aida.drop(columns=[i+'_correct_bi' for i in ['avg_ner_per',
'avg_ner_loc',
'avg_ner_org',
'avg_ner_misc']])
aida = aida.drop(columns=[i+'_correct_cross' for i in ['avg_ner_per',
'avg_ner_loc',
'avg_ner_org',
'avg_ner_misc']])
```

```{python}
avg_ner_types = aida[aida['ner_per'].notna()].query('label_id != 0').groupby('label_id')[['ner_per', 'ner_loc', 'ner_org', 'ner_misc']].mean()
avg_ner_types = avg_ner_types.rename(columns={
        'ner_per': 'avg_ner_per',
        'ner_loc': 'avg_ner_loc',
        'ner_org': 'avg_ner_org',
        'ner_misc': 'avg_ner_misc',
    })
```

```{python}
avg_ner_types
```

```{python}
aida[['label_id', 'cross_labels', 'y']]
```

```{python}
wikipedia_id2local_id[1411702]
```

```{python}
import numpy as np
```

```{python}
local_ids = np.array(list(map(lambda x: wikipedia_id2local_id[int(x)] if int(x) in wikipedia_id2local_id else None, avg_ner_types.index.tolist())))
```

```{python}
avg_ner_types['local_id'] = local_ids
```

```{python}
avg_ner_types['local_id'].isna().sum()/ avg_ner_types.shape[0]
```

```{python}
aida['cross_best_candidate'].isin(avg_ner_types['local_id']).sum()
```

```{python}
aida['cross_best_candidate'].isin(avg_ner_types['local_id']).sum()/aida.shape[0]
```

```{python}
aida['y'].value_counts()
```

```{python}
aida[aida['cross_best_candidate'].isin(avg_ner_types['local_id'])]['y'].value_counts()
```

```{python}
_temp_aida = whole5[whole5['src'].apply(lambda x: 'AIDA' in x)]
```

```{python}
_temp_aida[_temp_aida['cross_best_candidate'].isin(avg_ner_types['local_id'])]['y'].value_counts()
```

```{python}
aida[aida['ner_per'].notna()][aida['cross_best_candidate'].isin(avg_ner_types['local_id'])]['y'].value_counts()
```

```{python}
aida.shape
```

```{python}
aida_ner_avg = aida.merge(
    avg_ner_types.rename(columns={
        'avg_ner_per': 'avg_ner_per_correct',
        'avg_ner_loc': 'avg_ner_loc_correct',
        'avg_ner_org': 'avg_ner_org_correct',
        'avg_ner_misc': 'avg_ner_misc_correct',
    }, how='left', left_on='cross_best_candidate', right_on='local_id', suffixes=('', ''))
```

```{python}
aida_ner_avg.columns
```

```{python}
aida_ner_avg.shape
```

```{python}
whole5.shape
```

```{python}
train_df = train_df.join(avg_ner_types_correct, on='cross_best_candidate')
train_df = train_df.join(avg_ner_types, on='cross_best_candidate')
```

```{python}
aida_ner_avg.columns
```
